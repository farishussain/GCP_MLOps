{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20507085",
   "metadata": {},
   "source": [
    "# Model Deployment with Vertex AI\n",
    "\n",
    "This notebook demonstrates comprehensive model deployment using Google Cloud Vertex AI, including endpoint creation, model deployment, traffic management, and monitoring.\n",
    "\n",
    "## Learning Objectives\n",
    "- Deploy trained models to Vertex AI endpoints\n",
    "- Manage traffic splitting for A/B testing\n",
    "- Monitor deployment health and performance\n",
    "- Implement automated deployment pipelines\n",
    "- Handle deployment scaling and optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106334f4",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2865087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Import our deployment modules\n",
    "from src.deployment import (\n",
    "    EndpointManager,\n",
    "    ModelServingManager,\n",
    "    DeploymentMonitor,\n",
    "    ModelDeploymentConfig,\n",
    "    HealthChecker,\n",
    "    PerformanceMonitor,\n",
    "    get_deployment_recommendations\n",
    ")\n",
    "\n",
    "from src.config import Config\n",
    "from src.utils import setup_logging\n",
    "\n",
    "# Setup logging\n",
    "logger = setup_logging()\n",
    "\n",
    "print(\"ğŸš€ Model Deployment Environment Ready!\")\n",
    "print(f\"ğŸ“ Project Root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e2055a",
   "metadata": {},
   "source": [
    "## Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747e1d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from Google Cloud\n",
    "import subprocess\n",
    "from google.cloud import storage, aiplatform\n",
    "import google.auth\n",
    "\n",
    "print(\"â˜ï¸ Setting up Google Cloud Vertex AI deployment...\")\n",
    "\n",
    "try:\n",
    "    # Get project ID from gcloud \n",
    "    result = subprocess.run(['gcloud', 'config', 'get-value', 'project'], \n",
    "                          capture_output=True, text=True)\n",
    "    if result.returncode == 0 and result.stdout.strip():\n",
    "        PROJECT_ID = result.stdout.strip()\n",
    "    else:\n",
    "        PROJECT_ID = \"mlops-295610\"  # fallback\n",
    "    \n",
    "    LOCATION = \"us-central1\"\n",
    "    MODELS_BUCKET = f\"{PROJECT_ID}-mlops-models\"\n",
    "    STAGING_BUCKET = f\"{PROJECT_ID}-vertex-ai-staging\"\n",
    "    \n",
    "    print(f\"ğŸ”§ Deployment Configuration:\")\n",
    "    print(f\"   ğŸ“‹ Project ID: {PROJECT_ID}\")\n",
    "    print(f\"   ğŸŒ Location: {LOCATION}\")\n",
    "    print(f\"   \udd16 Models Bucket: {MODELS_BUCKET}\")\n",
    "    print(f\"   ğŸ—‚ï¸ Staging Bucket: {STAGING_BUCKET}\")\n",
    "    \n",
    "    # Initialize Vertex AI\n",
    "    aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
    "    \n",
    "    # Initialize storage client\n",
    "    credentials, project = google.auth.default()\n",
    "    storage_client = storage.Client(project=PROJECT_ID)\n",
    "    \n",
    "    print(\"âœ… Google Cloud services initialized successfully\")\n",
    "    \n",
    "    # Validate configuration\n",
    "    if PROJECT_ID and PROJECT_ID != 'your-project-id':\n",
    "        print(\"âœ… Configuration validated - ready for deployment\")\n",
    "    else:\n",
    "        print(\"âš ï¸  Warning: Using fallback project ID\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ GCP initialization error: {e}\")\n",
    "    print(\"   Some deployment features may not work without proper GCP setup\")\n",
    "    \n",
    "    # Fallback configuration \n",
    "    PROJECT_ID = \"mlops-295610\"\n",
    "    LOCATION = \"us-central1\"\n",
    "    MODELS_BUCKET = f\"{PROJECT_ID}-mlops-models\"\n",
    "    STAGING_BUCKET = f\"{PROJECT_ID}-vertex-ai-staging\"\n",
    "    \n",
    "    print(f\"ğŸ”§ Using fallback configuration:\")\n",
    "    print(f\"   ğŸ“‹ Project ID: {PROJECT_ID}\")\n",
    "    print(f\"   ğŸŒ Location: {LOCATION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9247c36d",
   "metadata": {},
   "source": [
    "## Initialize Deployment Managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ce1348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI deployment services\n",
    "print(\"ğŸš€ Initializing Vertex AI deployment services...\")\n",
    "\n",
    "try:\n",
    "    # Native Vertex AI services (no custom wrapper classes needed)\n",
    "    print(\"âœ… Using native Google Cloud Vertex AI services:\")\n",
    "    print(\"   ğŸ¤– Vertex AI Models - for model registration\")  \n",
    "    print(\"   ğŸ”— Vertex AI Endpoints - for serving\")\n",
    "    print(\"   ğŸ“Š Vertex AI Monitoring - for health checks\")\n",
    "    print(\"   ğŸ”„ Vertex AI Pipelines - for deployment automation\")\n",
    "    \n",
    "    # Test Vertex AI connection\n",
    "    models = aiplatform.Model.list(filter=f'project_id=\"{PROJECT_ID}\"')\n",
    "    endpoints = aiplatform.Endpoint.list(filter=f'project_id=\"{PROJECT_ID}\"')\n",
    "    \n",
    "    print(f\"   ğŸ“‹ Found {len(models)} existing models in project\")\n",
    "    print(f\"   ğŸ“¡ Found {len(endpoints)} existing endpoints in project\") \n",
    "    \n",
    "    # Check for required buckets\n",
    "    try:\n",
    "        bucket = storage_client.bucket(MODELS_BUCKET)\n",
    "        if bucket.exists():\n",
    "            print(f\"   âœ… Models bucket exists: {MODELS_BUCKET}\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸ Models bucket not found: {MODELS_BUCKET}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ Bucket check failed: {e}\")\n",
    "    \n",
    "    vertex_ai_ready = True\n",
    "    print(\"âœ… Vertex AI services are ready for deployment\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error connecting to Vertex AI: {e}\")\n",
    "    print(\"ğŸ’¡ Possible issues:\")\n",
    "    print(\"   - Google Cloud credentials not configured\")\n",
    "    print(\"   - Vertex AI API not enabled\")\n",
    "    print(\"   - Insufficient permissions\")\n",
    "    print(\"   - Project ID incorrect\")\n",
    "    \n",
    "    vertex_ai_ready = False\n",
    "    \n",
    "print(f\"\\nğŸ“Š Deployment Environment Status:\")\n",
    "print(f\"   Vertex AI Ready: {'âœ…' if vertex_ai_ready else 'âŒ'}\")\n",
    "print(f\"   Project: {PROJECT_ID}\")\n",
    "print(f\"   Region: {LOCATION}\")\n",
    "\n",
    "if not vertex_ai_ready:\n",
    "    print(f\"\\nğŸ”§ To fix Vertex AI connection:\")\n",
    "    print(f\"   1. Run: gcloud auth application-default login\")\n",
    "    print(f\"   2. Run: gcloud services enable aiplatform.googleapis.com\")\n",
    "    print(f\"   3. Verify project: gcloud config get-value project\")\n",
    "else:\n",
    "    print(f\"\\nğŸš€ Ready to deploy models to Vertex AI!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd3c1c5",
   "metadata": {},
   "source": [
    "## 1. Deployment Planning and Recommendations\n",
    "\n",
    "Before deploying a model, let's get deployment recommendations based on our requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be542fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define real deployment requirements for Vertex AI\n",
    "print(\"ğŸ“‹ Planning Vertex AI Model Deployment...\")\n",
    "\n",
    "# Define deployment requirements\n",
    "deployment_requirements = {\n",
    "    'model_type': 'sklearn',\n",
    "    'framework': 'scikit-learn',\n",
    "    'expected_qps': 10,  # queries per second\n",
    "    'latency_requirement_ms': 500,  # max response time\n",
    "    'availability_requirement': 99.9,  # uptime percentage\n",
    "    'traffic_pattern': 'steady',  # steady, bursty, or batch\n",
    "    'budget_category': 'development',  # development, staging, production\n",
    "    'auto_scaling': True,\n",
    "    'monitoring_required': True\n",
    "}\n",
    "\n",
    "print(\"ğŸ¯ Deployment Requirements:\")\n",
    "for key, value in deployment_requirements.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Get deployment recommendations based on requirements\n",
    "def get_vertex_ai_deployment_recommendations(requirements):\n",
    "    \"\"\"Generate Vertex AI deployment recommendations\"\"\"\n",
    "    \n",
    "    recommendations = {\n",
    "        'machine_type': 'n1-standard-2',  # 2 vCPU, 7.5GB RAM\n",
    "        'min_replica_count': 1,\n",
    "        'max_replica_count': 3,\n",
    "        'deployment_strategy': 'blue_green',\n",
    "        'traffic_allocation': {'new_version': 100},\n",
    "        'serving_container': 'us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest',\n",
    "        'estimated_cost_per_month_usd': 0,\n",
    "        'recommended_monitoring': []\n",
    "    }\n",
    "    \n",
    "    # Adjust based on QPS requirements\n",
    "    if requirements['expected_qps'] <= 10:\n",
    "        recommendations['machine_type'] = 'n1-standard-2'\n",
    "        recommendations['estimated_cost_per_month_usd'] = 50\n",
    "    elif requirements['expected_qps'] <= 100:\n",
    "        recommendations['machine_type'] = 'n1-standard-4'\n",
    "        recommendations['max_replica_count'] = 5\n",
    "        recommendations['estimated_cost_per_month_usd'] = 150\n",
    "    else:\n",
    "        recommendations['machine_type'] = 'n1-standard-8'\n",
    "        recommendations['max_replica_count'] = 10\n",
    "        recommendations['estimated_cost_per_month_usd'] = 400\n",
    "    \n",
    "    # Add monitoring recommendations\n",
    "    if requirements['monitoring_required']:\n",
    "        recommendations['recommended_monitoring'] = [\n",
    "            'Request latency tracking',\n",
    "            'Error rate monitoring', \n",
    "            'Resource utilization alerts',\n",
    "            'Model drift detection',\n",
    "            'Traffic pattern analysis'\n",
    "        ]\n",
    "    \n",
    "    # Adjust for budget\n",
    "    if requirements['budget_category'] == 'development':\n",
    "        recommendations['min_replica_count'] = 0  # Allow scale to zero\n",
    "        recommendations['estimated_cost_per_month_usd'] *= 0.3  # Dev discount\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Get recommendations\n",
    "recommendations = get_vertex_ai_deployment_recommendations(deployment_requirements)\n",
    "\n",
    "print(f\"\\nğŸ¯ Vertex AI Deployment Recommendations:\")\n",
    "print(f\"   \udda5ï¸  Machine Type: {recommendations['machine_type']}\")\n",
    "print(f\"   ğŸ“ˆ Replicas: {recommendations['min_replica_count']}-{recommendations['max_replica_count']}\")\n",
    "print(f\"   \udc33 Container: {recommendations['serving_container']}\")\n",
    "print(f\"   ğŸ’° Estimated Cost: ${recommendations['estimated_cost_per_month_usd']:.2f}/month\")\n",
    "print(f\"   \udcca Strategy: {recommendations['deployment_strategy']}\")\n",
    "\n",
    "if recommendations['recommended_monitoring']:\n",
    "    print(f\"   \udccb Monitoring:\")\n",
    "    for monitor in recommendations['recommended_monitoring']:\n",
    "        print(f\"      â€¢ {monitor}\")\n",
    "\n",
    "print(f\"\\nâœ… Deployment plan ready for implementation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36f00f9",
   "metadata": {},
   "source": [
    "## 2. Model Deployment Configuration\n",
    "\n",
    "Let's create a deployment configuration for our trained Iris classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247beb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create deployment configuration\n",
    "deployment_config = ModelDeploymentConfig(\n",
    "    model_id=\"iris-classifier-v1\",  # This would be the actual model resource name\n",
    "    endpoint_display_name=\"iris-classifier-endpoint\",\n",
    "    machine_type=recommendations['machine_type'],\n",
    "    min_replica_count=recommendations['min_replica_count'],\n",
    "    max_replica_count=recommendations['max_replica_count'],\n",
    "    traffic_percentage=100,\n",
    "    deployed_model_display_name=\"iris-classifier-v1-deployment\",\n",
    "    enable_container_logging=True,\n",
    "    enable_access_logging=True\n",
    ")\n",
    "\n",
    "print(\"âš™ï¸  Deployment Configuration Created:\")\n",
    "print(f\"   ğŸ”— Model ID: {deployment_config.model_id}\")\n",
    "print(f\"   ğŸ“¡ Endpoint Name: {deployment_config.endpoint_display_name}\")\n",
    "print(f\"   ğŸ’» Machine Type: {deployment_config.machine_type}\")\n",
    "print(f\"   ğŸ“ˆ Replica Range: {deployment_config.min_replica_count}-{deployment_config.max_replica_count}\")\n",
    "print(f\"   ğŸš¦ Traffic: {deployment_config.traffic_percentage}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8d3f7f",
   "metadata": {},
   "source": [
    "## 3. Endpoint Creation (Simulation)\n",
    "\n",
    "Let's simulate creating a Vertex AI endpoint for our model deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfad0ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate endpoint creation\n",
    "print(\"ğŸ”„ Creating Vertex AI Endpoint...\")\n",
    "print(\"   This would normally create an actual endpoint in Google Cloud\")\n",
    "\n",
    "# In a real scenario, this would create an actual endpoint\n",
    "endpoint_name = f\"iris-classifier-endpoint-{int(time.time())}\"\n",
    "simulated_endpoint_id = f\"projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/12345\"\n",
    "\n",
    "print(f\"\\nâœ… Endpoint Created (Simulated):\")\n",
    "print(f\"   ğŸ†” Resource Name: {simulated_endpoint_id}\")\n",
    "print(f\"   ğŸ“± Display Name: {endpoint_name}\")\n",
    "print(f\"   ğŸŒ Location: {LOCATION}\")\n",
    "print(f\"   ğŸ”— URL: https://{LOCATION}-aiplatform.googleapis.com/v1/{simulated_endpoint_id}\")\n",
    "\n",
    "# Store for later use\n",
    "ENDPOINT_ID = simulated_endpoint_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec60d2e",
   "metadata": {},
   "source": [
    "## 4. Model Deployment (Simulation)\n",
    "\n",
    "Now let's deploy our model to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa4cd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate model deployment\n",
    "print(\"ğŸš€ Deploying Model to Endpoint...\")\n",
    "print(\"   This process typically takes 10-20 minutes in production\")\n",
    "\n",
    "# Simulate deployment progress\n",
    "deployment_steps = [\n",
    "    \"ğŸ“¦ Preparing model artifacts\",\n",
    "    \"ğŸ—ï¸  Creating compute resources\",\n",
    "    \"ğŸ“¥ Loading model into memory\",\n",
    "    \"ğŸ”§ Configuring serving infrastructure\",\n",
    "    \"ğŸ§ª Running health checks\",\n",
    "    \"ğŸš¦ Configuring traffic routing\"\n",
    "]\n",
    "\n",
    "for i, step in enumerate(deployment_steps, 1):\n",
    "    print(f\"   [{i}/{len(deployment_steps)}] {step}\")\n",
    "    time.sleep(0.5)  # Simulate processing time\n",
    "\n",
    "print(\"\\nâœ… Model Deployment Complete!\")\n",
    "print(f\"   ğŸ¯ Model: iris-classifier-v1\")\n",
    "print(f\"   ğŸ“¡ Endpoint: {endpoint_name}\")\n",
    "print(f\"   ğŸš¦ Traffic: 100%\")\n",
    "print(f\"   ğŸ“Š Status: READY\")\n",
    "\n",
    "# Create deployment info\n",
    "deployment_info = {\n",
    "    'endpoint_id': ENDPOINT_ID,\n",
    "    'model_id': deployment_config.model_id,\n",
    "    'status': 'DEPLOYED',\n",
    "    'created_time': datetime.now().isoformat(),\n",
    "    'machine_type': deployment_config.machine_type,\n",
    "    'replica_count': deployment_config.min_replica_count\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ“‹ Deployment Details:\")\n",
    "for key, value in deployment_info.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39b6911",
   "metadata": {},
   "source": [
    "## 5. Making Predictions\n",
    "\n",
    "Let's test our deployed model with some sample predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65bebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample prediction instances\n",
    "sample_instances = [\n",
    "    {\n",
    "        \"sepal_length\": 5.1,\n",
    "        \"sepal_width\": 3.5,\n",
    "        \"petal_length\": 1.4,\n",
    "        \"petal_width\": 0.2\n",
    "    },\n",
    "    {\n",
    "        \"sepal_length\": 6.7,\n",
    "        \"sepal_width\": 3.1,\n",
    "        \"petal_length\": 4.7,\n",
    "        \"petal_width\": 1.5\n",
    "    },\n",
    "    {\n",
    "        \"sepal_length\": 7.3,\n",
    "        \"sepal_width\": 2.9,\n",
    "        \"petal_length\": 6.3,\n",
    "        \"petal_width\": 1.8\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª Testing Model Predictions:\")\n",
    "print(f\"   ğŸ“Š Sample Instances: {len(sample_instances)}\")\n",
    "print(\"   ğŸ”„ Sending prediction requests...\")\n",
    "\n",
    "# Simulate predictions\n",
    "simulated_predictions = [\n",
    "    {\"species\": \"setosa\", \"confidence\": 0.98},\n",
    "    {\"species\": \"versicolor\", \"confidence\": 0.89},\n",
    "    {\"species\": \"virginica\", \"confidence\": 0.95}\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ“ˆ Prediction Results:\")\n",
    "for i, (instance, prediction) in enumerate(zip(sample_instances, simulated_predictions), 1):\n",
    "    print(f\"   [{i}] Input: {instance}\")\n",
    "    print(f\"       Output: {prediction['species']} (confidence: {prediction['confidence']:.2f})\")\n",
    "    print()\n",
    "\n",
    "print(\"âœ… Predictions completed successfully!\")\n",
    "\n",
    "# Calculate latency simulation\n",
    "latency_ms = np.random.normal(150, 30)  # Simulate ~150ms average latency\n",
    "print(f\"â±ï¸  Average Latency: {latency_ms:.1f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42945e86",
   "metadata": {},
   "source": [
    "## 6. Health Monitoring\n",
    "\n",
    "Let's set up health monitoring for our deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7052a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize health checker\n",
    "health_checker = HealthChecker(endpoint_manager)\n",
    "\n",
    "print(\"ğŸ¥ Running Health Checks...\")\n",
    "\n",
    "# Simulate health check\n",
    "health_check_results = {\n",
    "    'endpoint_name': ENDPOINT_ID,\n",
    "    'status': 'healthy',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'latency_ms': latency_ms,\n",
    "    'response_code': 200,\n",
    "    'deployed_models_count': 1,\n",
    "    'error_message': None\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ“Š Health Check Results:\")\n",
    "print(f\"   ğŸŸ¢ Status: {health_check_results['status'].upper()}\")\n",
    "print(f\"   â±ï¸  Latency: {health_check_results['latency_ms']:.1f}ms\")\n",
    "print(f\"   ğŸ”¢ Response Code: {health_check_results['response_code']}\")\n",
    "print(f\"   ğŸ¤– Deployed Models: {health_check_results['deployed_models_count']}\")\n",
    "print(f\"   ğŸ•’ Last Check: {health_check_results['timestamp']}\")\n",
    "\n",
    "# Health status interpretation\n",
    "if health_check_results['status'] == 'healthy':\n",
    "    print(\"\\nâœ… Endpoint is healthy and ready to serve requests\")\n",
    "elif health_check_results['status'] == 'warning':\n",
    "    print(\"\\nâš ï¸  Endpoint has performance issues but is operational\")\n",
    "else:\n",
    "    print(\"\\nâŒ Endpoint is experiencing critical issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaf13f9",
   "metadata": {},
   "source": [
    "## 7. Performance Monitoring\n",
    "\n",
    "Let's monitor the performance metrics of our deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3584afee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize performance monitor\n",
    "performance_monitor = PerformanceMonitor(PROJECT_ID, LOCATION)\n",
    "\n",
    "print(\"ğŸ“Š Collecting Performance Metrics...\")\n",
    "\n",
    "# Simulate performance metrics collection\n",
    "performance_metrics = {\n",
    "    'endpoint_name': ENDPOINT_ID,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'request_count': 1247,\n",
    "    'error_count': 12,\n",
    "    'average_latency_ms': 145.3,\n",
    "    'p95_latency_ms': 289.7,\n",
    "    'p99_latency_ms': 456.2,\n",
    "    'throughput_qps': 42.5,\n",
    "    'cpu_utilization': 34.2,\n",
    "    'memory_utilization': 28.7,\n",
    "    'error_rate': 0.96\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Performance Metrics (Last 5 minutes):\")\n",
    "print(f\"   ğŸ“Š Requests: {performance_metrics['request_count']:,}\")\n",
    "print(f\"   âŒ Errors: {performance_metrics['error_count']} ({performance_metrics['error_rate']:.2f}%)\")\n",
    "print(f\"   â±ï¸  Avg Latency: {performance_metrics['average_latency_ms']:.1f}ms\")\n",
    "print(f\"   ğŸ“Š P95 Latency: {performance_metrics['p95_latency_ms']:.1f}ms\")\n",
    "print(f\"   ğŸ“Š P99 Latency: {performance_metrics['p99_latency_ms']:.1f}ms\")\n",
    "print(f\"   ğŸš€ Throughput: {performance_metrics['throughput_qps']:.1f} QPS\")\n",
    "print(f\"   ğŸ’» CPU Usage: {performance_metrics['cpu_utilization']:.1f}%\")\n",
    "print(f\"   ğŸ§  Memory Usage: {performance_metrics['memory_utilization']:.1f}%\")\n",
    "\n",
    "# Performance assessment\n",
    "if performance_metrics['error_rate'] < 1.0:\n",
    "    error_status = \"ğŸŸ¢ Excellent\"\n",
    "elif performance_metrics['error_rate'] < 5.0:\n",
    "    error_status = \"ğŸŸ¡ Acceptable\"\n",
    "else:\n",
    "    error_status = \"ğŸ”´ Critical\"\n",
    "\n",
    "if performance_metrics['average_latency_ms'] < 200:\n",
    "    latency_status = \"ğŸŸ¢ Fast\"\n",
    "elif performance_metrics['average_latency_ms'] < 500:\n",
    "    latency_status = \"ğŸŸ¡ Moderate\"\n",
    "else:\n",
    "    latency_status = \"ğŸ”´ Slow\"\n",
    "\n",
    "print(f\"\\nğŸ¯ Performance Assessment:\")\n",
    "print(f\"   ğŸ“‰ Error Rate: {error_status}\")\n",
    "print(f\"   âš¡ Latency: {latency_status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a78e2f",
   "metadata": {},
   "source": [
    "## 8. A/B Testing Setup\n",
    "\n",
    "Let's demonstrate how to set up A/B testing with traffic splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c915cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”¬ Setting up A/B Testing with Traffic Splitting...\")\n",
    "\n",
    "# Simulate deploying a new model version\n",
    "print(\"\\nğŸ“¦ Deploying Model Version 2 (Champion/Challenger):\")\n",
    "\n",
    "# Current deployment (Champion)\n",
    "champion_model = {\n",
    "    'model_id': 'iris-classifier-v1',\n",
    "    'version': 'v1.0',\n",
    "    'traffic_percentage': 80,\n",
    "    'deployment_time': (datetime.now() - timedelta(days=7)).isoformat()\n",
    "}\n",
    "\n",
    "# New deployment (Challenger)\n",
    "challenger_model = {\n",
    "    'model_id': 'iris-classifier-v2',\n",
    "    'version': 'v2.0',\n",
    "    'traffic_percentage': 20,\n",
    "    'deployment_time': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "print(f\"   ğŸ† Champion Model:\")\n",
    "print(f\"      ğŸ†” ID: {champion_model['model_id']}\")\n",
    "print(f\"      ğŸ“Š Traffic: {champion_model['traffic_percentage']}%\")\n",
    "print(f\"      ğŸ“… Deployed: {champion_model['deployment_time'][:10]}\")\n",
    "\n",
    "print(f\"   ğŸ¥Š Challenger Model:\")\n",
    "print(f\"      ğŸ†” ID: {challenger_model['model_id']}\")\n",
    "print(f\"      ğŸ“Š Traffic: {challenger_model['traffic_percentage']}%\")\n",
    "print(f\"      ğŸ“… Deployed: {challenger_model['deployment_time'][:10]}\")\n",
    "\n",
    "# Traffic split configuration\n",
    "traffic_split = {\n",
    "    champion_model['model_id']: champion_model['traffic_percentage'],\n",
    "    challenger_model['model_id']: challenger_model['traffic_percentage']\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸš¦ Traffic Split Configuration:\")\n",
    "for model_id, percentage in traffic_split.items():\n",
    "    print(f\"   {model_id}: {percentage}%\")\n",
    "\n",
    "print(\"\\nâœ… A/B Testing setup complete!\")\n",
    "print(\"   ğŸ“Š 20% of traffic will test the new model\")\n",
    "print(\"   ğŸ“ˆ Performance will be compared over time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef8446f",
   "metadata": {},
   "source": [
    "## 9. A/B Testing Results Analysis\n",
    "\n",
    "Let's analyze the performance comparison between model versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e866d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Simulate A/B testing results after 24 hours\n",
    "print(\"ğŸ“Š A/B Testing Results Analysis (24 hour period):\")\n",
    "\n",
    "# Simulate performance data for both models\n",
    "np.random.seed(42)  # For reproducible results\n",
    "\n",
    "# Champion model performance (baseline)\n",
    "champion_latency = np.random.normal(145, 25, 1000)  # Mean: 145ms, Std: 25ms\n",
    "champion_accuracy = np.random.normal(0.945, 0.02, 1000)  # Mean: 94.5% accuracy\n",
    "champion_error_rate = 0.8  # 0.8% error rate\n",
    "\n",
    "# Challenger model performance (potentially improved)\n",
    "challenger_latency = np.random.normal(120, 20, 250)  # Mean: 120ms, Std: 20ms (improved)\n",
    "challenger_accuracy = np.random.normal(0.955, 0.015, 250)  # Mean: 95.5% accuracy (improved)\n",
    "challenger_error_rate = 0.5  # 0.5% error rate (improved)\n",
    "\n",
    "# Calculate statistics\n",
    "results_comparison = {\n",
    "    'champion': {\n",
    "        'model_id': champion_model['model_id'],\n",
    "        'traffic_percentage': champion_model['traffic_percentage'],\n",
    "        'requests': len(champion_latency),\n",
    "        'avg_latency_ms': np.mean(champion_latency),\n",
    "        'p95_latency_ms': np.percentile(champion_latency, 95),\n",
    "        'avg_accuracy': np.mean(champion_accuracy),\n",
    "        'error_rate': champion_error_rate\n",
    "    },\n",
    "    'challenger': {\n",
    "        'model_id': challenger_model['model_id'],\n",
    "        'traffic_percentage': challenger_model['traffic_percentage'],\n",
    "        'requests': len(challenger_latency),\n",
    "        'avg_latency_ms': np.mean(challenger_latency),\n",
    "        'p95_latency_ms': np.percentile(challenger_latency, 95),\n",
    "        'avg_accuracy': np.mean(challenger_accuracy),\n",
    "        'error_rate': challenger_error_rate\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display results\n",
    "print(\"\\nğŸ“ˆ Performance Comparison:\")\n",
    "print(f\"\\nğŸ† Champion Model ({champion_model['model_id']}):\")\n",
    "print(f\"   ğŸ“Š Requests: {results_comparison['champion']['requests']:,}\")\n",
    "print(f\"   â±ï¸  Avg Latency: {results_comparison['champion']['avg_latency_ms']:.1f}ms\")\n",
    "print(f\"   ğŸ“Š P95 Latency: {results_comparison['champion']['p95_latency_ms']:.1f}ms\")\n",
    "print(f\"   ğŸ¯ Accuracy: {results_comparison['champion']['avg_accuracy']*100:.2f}%\")\n",
    "print(f\"   âŒ Error Rate: {results_comparison['champion']['error_rate']:.1f}%\")\n",
    "\n",
    "print(f\"\\nğŸ¥Š Challenger Model ({challenger_model['model_id']}):\")\n",
    "print(f\"   ğŸ“Š Requests: {results_comparison['challenger']['requests']:,}\")\n",
    "print(f\"   â±ï¸  Avg Latency: {results_comparison['challenger']['avg_latency_ms']:.1f}ms\")\n",
    "print(f\"   ğŸ“Š P95 Latency: {results_comparison['challenger']['p95_latency_ms']:.1f}ms\")\n",
    "print(f\"   ğŸ¯ Accuracy: {results_comparison['challenger']['avg_accuracy']*100:.2f}%\")\n",
    "print(f\"   âŒ Error Rate: {results_comparison['challenger']['error_rate']:.1f}%\")\n",
    "\n",
    "# Calculate improvements\n",
    "latency_improvement = ((results_comparison['champion']['avg_latency_ms'] - \n",
    "                       results_comparison['challenger']['avg_latency_ms']) / \n",
    "                      results_comparison['champion']['avg_latency_ms'] * 100)\n",
    "\n",
    "accuracy_improvement = ((results_comparison['challenger']['avg_accuracy'] - \n",
    "                        results_comparison['champion']['avg_accuracy']) / \n",
    "                       results_comparison['champion']['avg_accuracy'] * 100)\n",
    "\n",
    "error_improvement = ((results_comparison['champion']['error_rate'] - \n",
    "                     results_comparison['challenger']['error_rate']) / \n",
    "                    results_comparison['champion']['error_rate'] * 100)\n",
    "\n",
    "print(f\"\\nğŸ“Š Performance Improvements:\")\n",
    "print(f\"   âš¡ Latency: {latency_improvement:+.1f}% (better is negative)\")\n",
    "print(f\"   ğŸ¯ Accuracy: {accuracy_improvement:+.2f}% (better is positive)\")\n",
    "print(f\"   âŒ Error Rate: {error_improvement:+.1f}% improvement\")\n",
    "\n",
    "# Decision making\n",
    "if latency_improvement > 10 and accuracy_improvement > 0.5 and error_improvement > 20:\n",
    "    recommendation = \"ğŸš€ PROMOTE CHALLENGER - Significant improvements across all metrics\"\n",
    "elif latency_improvement > 0 and accuracy_improvement > 0:\n",
    "    recommendation = \"âœ… PROMOTE CHALLENGER - Moderate improvements observed\"\n",
    "else:\n",
    "    recommendation = \"âš ï¸  KEEP CHAMPION - No significant improvement\"\n",
    "\n",
    "print(f\"\\nğŸ¯ Recommendation: {recommendation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fde7386",
   "metadata": {},
   "source": [
    "## 10. Automated Deployment Pipeline\n",
    "\n",
    "Let's create an automated deployment pipeline configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b10c614",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¤– Setting up Automated Deployment Pipeline...\")\n",
    "\n",
    "# Define deployment pipeline configuration\n",
    "deployment_pipeline = {\n",
    "    'name': 'iris-classifier-deployment-pipeline',\n",
    "    'version': '1.0',\n",
    "    'stages': [\n",
    "        {\n",
    "            'name': 'model-validation',\n",
    "            'type': 'validation',\n",
    "            'criteria': {\n",
    "                'min_accuracy': 0.90,\n",
    "                'max_latency_ms': 500,\n",
    "                'required_tests': ['unit_tests', 'integration_tests', 'performance_tests']\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'staging-deployment',\n",
    "            'type': 'deployment',\n",
    "            'environment': 'staging',\n",
    "            'config': {\n",
    "                'machine_type': 'n1-standard-2',\n",
    "                'min_replicas': 1,\n",
    "                'max_replicas': 3,\n",
    "                'traffic_percentage': 100\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'staging-testing',\n",
    "            'type': 'testing',\n",
    "            'duration_minutes': 30,\n",
    "            'tests': [\n",
    "                'smoke_tests',\n",
    "                'load_tests',\n",
    "                'regression_tests'\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            'name': 'canary-deployment',\n",
    "            'type': 'deployment',\n",
    "            'environment': 'production',\n",
    "            'config': {\n",
    "                'machine_type': 'n1-standard-4',\n",
    "                'min_replicas': 2,\n",
    "                'max_replicas': 10,\n",
    "                'traffic_percentage': 10,\n",
    "                'duration_minutes': 60\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'full-deployment',\n",
    "            'type': 'deployment',\n",
    "            'environment': 'production',\n",
    "            'trigger': 'manual_approval',\n",
    "            'config': {\n",
    "                'traffic_percentage': 100,\n",
    "                'rollback_threshold': {\n",
    "                    'error_rate': 2.0,\n",
    "                    'latency_ms': 1000\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    'monitoring': {\n",
    "        'health_checks': True,\n",
    "        'performance_monitoring': True,\n",
    "        'alerting': True,\n",
    "        'dashboard': True\n",
    "    },\n",
    "    'rollback': {\n",
    "        'automatic': True,\n",
    "        'conditions': {\n",
    "            'error_rate_threshold': 5.0,\n",
    "            'latency_threshold_ms': 2000,\n",
    "            'availability_threshold': 99.0\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"ğŸ“‹ Pipeline Configuration:\")\n",
    "print(f\"   ğŸ“ Name: {deployment_pipeline['name']}\")\n",
    "print(f\"   ğŸ”¢ Version: {deployment_pipeline['version']}\")\n",
    "print(f\"   ğŸš€ Stages: {len(deployment_pipeline['stages'])}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Deployment Stages:\")\n",
    "for i, stage in enumerate(deployment_pipeline['stages'], 1):\n",
    "    print(f\"   [{i}] {stage['name']} ({stage['type']})\")\n",
    "    \n",
    "    if stage['type'] == 'deployment':\n",
    "        config = stage.get('config', {})\n",
    "        print(f\"       ğŸŒ Environment: {stage.get('environment', 'N/A')}\")\n",
    "        print(f\"       ğŸš¦ Traffic: {config.get('traffic_percentage', 0)}%\")\n",
    "        if 'duration_minutes' in stage:\n",
    "            print(f\"       â° Duration: {stage['duration_minutes']} minutes\")\n",
    "    \n",
    "    elif stage['type'] == 'validation':\n",
    "        criteria = stage.get('criteria', {})\n",
    "        print(f\"       ğŸ¯ Min Accuracy: {criteria.get('min_accuracy', 0)*100:.0f}%\")\n",
    "        print(f\"       â±ï¸  Max Latency: {criteria.get('max_latency_ms', 0)}ms\")\n",
    "    \n",
    "    elif stage['type'] == 'testing':\n",
    "        print(f\"       â° Duration: {stage.get('duration_minutes', 0)} minutes\")\n",
    "        print(f\"       ğŸ§ª Tests: {len(stage.get('tests', []))}\")\n",
    "\n",
    "print(f\"\\nğŸ”§ Pipeline Features:\")\n",
    "print(f\"   ğŸ“Š Monitoring: {'âœ…' if deployment_pipeline['monitoring']['health_checks'] else 'âŒ'}\")\n",
    "print(f\"   ğŸš¨ Alerting: {'âœ…' if deployment_pipeline['monitoring']['alerting'] else 'âŒ'}\")\n",
    "print(f\"   ğŸ”„ Auto Rollback: {'âœ…' if deployment_pipeline['rollback']['automatic'] else 'âŒ'}\")\n",
    "\n",
    "print(\"\\nâœ… Automated deployment pipeline configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0f9852",
   "metadata": {},
   "source": [
    "## 11. Monitoring Dashboard Simulation\n",
    "\n",
    "Let's create a monitoring dashboard view of our deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a491b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create monitoring dashboard data\n",
    "dashboard_data = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'time_range_hours': 24,\n",
    "    'endpoints': [\n",
    "        {\n",
    "            'name': 'iris-classifier-endpoint',\n",
    "            'display_name': 'Iris Classifier Endpoint',\n",
    "            'health_status': 'healthy',\n",
    "            'last_check_latency_ms': 142.3,\n",
    "            'deployed_models_count': 2,\n",
    "            'active_alerts_count': 0,\n",
    "            'traffic_split': {\n",
    "                'iris-classifier-v1': 80,\n",
    "                'iris-classifier-v2': 20\n",
    "            },\n",
    "            'performance_summary': {\n",
    "                'requests_24h': 28745,\n",
    "                'avg_latency_ms': 145.7,\n",
    "                'error_rate': 0.8,\n",
    "                'availability': 99.95\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'text-classifier-endpoint',\n",
    "            'display_name': 'Text Classifier Endpoint',\n",
    "            'health_status': 'warning',\n",
    "            'last_check_latency_ms': 340.1,\n",
    "            'deployed_models_count': 1,\n",
    "            'active_alerts_count': 1,\n",
    "            'traffic_split': {\n",
    "                'text-classifier-v1': 100\n",
    "            },\n",
    "            'performance_summary': {\n",
    "                'requests_24h': 12387,\n",
    "                'avg_latency_ms': 298.4,\n",
    "                'error_rate': 2.3,\n",
    "                'availability': 99.2\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    'overall_stats': {\n",
    "        'total_endpoints': 2,\n",
    "        'healthy_endpoints': 1,\n",
    "        'active_alerts': 1,\n",
    "        'total_requests_24h': 41132,\n",
    "        'overall_availability': 99.58\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"ğŸ“Š Deployment Monitoring Dashboard\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ğŸ“… Time Range: Last {dashboard_data['time_range_hours']} hours\")\n",
    "print(f\"ğŸ•’ Last Updated: {dashboard_data['timestamp'][:19]}\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Overall Statistics:\")\n",
    "stats = dashboard_data['overall_stats']\n",
    "print(f\"   ğŸ“¡ Total Endpoints: {stats['total_endpoints']}\")\n",
    "print(f\"   ğŸŸ¢ Healthy: {stats['healthy_endpoints']}/{stats['total_endpoints']}\")\n",
    "print(f\"   ğŸš¨ Active Alerts: {stats['active_alerts']}\")\n",
    "print(f\"   ğŸ“Š Total Requests: {stats['total_requests_24h']:,}\")\n",
    "print(f\"   âš¡ Availability: {stats['overall_availability']:.2f}%\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Endpoint Details:\")\n",
    "for endpoint in dashboard_data['endpoints']:\n",
    "    # Status indicator\n",
    "    status_icon = {\n",
    "        'healthy': 'ğŸŸ¢',\n",
    "        'warning': 'ğŸŸ¡', \n",
    "        'critical': 'ğŸ”´'\n",
    "    }.get(endpoint['health_status'], 'âšª')\n",
    "    \n",
    "    print(f\"\\n   {status_icon} {endpoint['display_name']}\")\n",
    "    print(f\"      ğŸ†” Name: {endpoint['name']}\")\n",
    "    print(f\"      ğŸ“Š Status: {endpoint['health_status'].upper()}\")\n",
    "    print(f\"      â±ï¸  Latency: {endpoint['last_check_latency_ms']:.1f}ms\")\n",
    "    print(f\"      ğŸ¤– Models: {endpoint['deployed_models_count']}\")\n",
    "    print(f\"      ğŸš¨ Alerts: {endpoint['active_alerts_count']}\")\n",
    "    \n",
    "    # Traffic split\n",
    "    print(f\"      ğŸš¦ Traffic Split:\")\n",
    "    for model, percentage in endpoint['traffic_split'].items():\n",
    "        print(f\"         {model}: {percentage}%\")\n",
    "    \n",
    "    # Performance summary\n",
    "    perf = endpoint['performance_summary']\n",
    "    print(f\"      ğŸ“ˆ Performance (24h):\")\n",
    "    print(f\"         ğŸ“Š Requests: {perf['requests_24h']:,}\")\n",
    "    print(f\"         â±ï¸  Avg Latency: {perf['avg_latency_ms']:.1f}ms\")\n",
    "    print(f\"         âŒ Error Rate: {perf['error_rate']:.1f}%\")\n",
    "    print(f\"         âš¡ Availability: {perf['availability']:.2f}%\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"âœ… Dashboard data refreshed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffed5154",
   "metadata": {},
   "source": [
    "## 12. Cost Analysis and Optimization\n",
    "\n",
    "Let's analyze the cost of our deployments and identify optimization opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c39f636",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ’° Deployment Cost Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Define cost components\n",
    "cost_analysis = {\n",
    "    'iris_classifier_endpoint': {\n",
    "        'machine_type': 'n1-standard-4',\n",
    "        'replicas': 3,\n",
    "        'hours_per_day': 24,\n",
    "        'cost_per_hour_per_replica': 0.19,\n",
    "        'prediction_requests_daily': 28745,\n",
    "        'cost_per_1000_predictions': 0.50\n",
    "    },\n",
    "    'text_classifier_endpoint': {\n",
    "        'machine_type': 'n1-standard-2',\n",
    "        'replicas': 2,\n",
    "        'hours_per_day': 24,\n",
    "        'cost_per_hour_per_replica': 0.095,\n",
    "        'prediction_requests_daily': 12387,\n",
    "        'cost_per_1000_predictions': 0.50\n",
    "    }\n",
    "}\n",
    "\n",
    "total_daily_cost = 0\n",
    "total_monthly_cost = 0\n",
    "\n",
    "for endpoint_name, costs in cost_analysis.items():\n",
    "    # Calculate compute costs\n",
    "    daily_compute_cost = (costs['replicas'] * \n",
    "                         costs['hours_per_day'] * \n",
    "                         costs['cost_per_hour_per_replica'])\n",
    "    \n",
    "    # Calculate prediction costs\n",
    "    daily_prediction_cost = (costs['prediction_requests_daily'] / 1000 * \n",
    "                            costs['cost_per_1000_predictions'])\n",
    "    \n",
    "    daily_total = daily_compute_cost + daily_prediction_cost\n",
    "    monthly_total = daily_total * 30\n",
    "    \n",
    "    print(f\"\\nğŸ“¡ {endpoint_name.replace('_', ' ').title()}:\")\n",
    "    print(f\"   ğŸ’» Machine Type: {costs['machine_type']}\")\n",
    "    print(f\"   ğŸ”¢ Replicas: {costs['replicas']}\")\n",
    "    print(f\"   ğŸ“Š Daily Requests: {costs['prediction_requests_daily']:,}\")\n",
    "    print(f\"   ğŸ’µ Daily Compute: ${daily_compute_cost:.2f}\")\n",
    "    print(f\"   ğŸ’µ Daily Predictions: ${daily_prediction_cost:.2f}\")\n",
    "    print(f\"   ğŸ’° Daily Total: ${daily_total:.2f}\")\n",
    "    print(f\"   ğŸ“… Monthly Total: ${monthly_total:.2f}\")\n",
    "    \n",
    "    total_daily_cost += daily_total\n",
    "    total_monthly_cost += monthly_total\n",
    "\n",
    "print(f\"\\nğŸ“Š Overall Cost Summary:\")\n",
    "print(f\"   ğŸ’° Total Daily Cost: ${total_daily_cost:.2f}\")\n",
    "print(f\"   ğŸ“… Total Monthly Cost: ${total_monthly_cost:.2f}\")\n",
    "print(f\"   ğŸ“ˆ Annual Cost: ${total_monthly_cost * 12:.2f}\")\n",
    "\n",
    "# Cost optimization recommendations\n",
    "print(f\"\\nğŸ¯ Cost Optimization Recommendations:\")\n",
    "\n",
    "optimizations = [\n",
    "    {\n",
    "        'strategy': 'Auto-scaling Configuration',\n",
    "        'description': 'Implement auto-scaling to reduce replicas during low traffic',\n",
    "        'potential_savings': '20-30%',\n",
    "        'implementation': 'Configure min/max replicas based on traffic patterns'\n",
    "    },\n",
    "    {\n",
    "        'strategy': 'Machine Type Optimization',\n",
    "        'description': 'Use smaller machine types for endpoints with low CPU usage',\n",
    "        'potential_savings': '15-25%',\n",
    "        'implementation': 'Monitor CPU/memory usage and downgrade if possible'\n",
    "    },\n",
    "    {\n",
    "        'strategy': 'Batch Prediction',\n",
    "        'description': 'Use batch prediction for non-real-time workloads',\n",
    "        'potential_savings': '40-60%',\n",
    "        'implementation': 'Identify workloads suitable for batch processing'\n",
    "    },\n",
    "    {\n",
    "        'strategy': 'Preemptible Instances',\n",
    "        'description': 'Use preemptible instances for development/staging',\n",
    "        'potential_savings': '60-70%',\n",
    "        'implementation': 'Apply to non-production environments'\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, opt in enumerate(optimizations, 1):\n",
    "    print(f\"\\n   [{i}] {opt['strategy']}\")\n",
    "    print(f\"       ğŸ“ {opt['description']}\")\n",
    "    print(f\"       ğŸ’° Potential Savings: {opt['potential_savings']}\")\n",
    "    print(f\"       ğŸ”§ Implementation: {opt['implementation']}\")\n",
    "\n",
    "# Calculate potential savings\n",
    "estimated_savings = total_monthly_cost * 0.30  # Assume 30% average savings\n",
    "print(f\"\\nğŸ’¡ Estimated Monthly Savings with Optimizations: ${estimated_savings:.2f}\")\n",
    "print(f\"ğŸ“Š Optimized Monthly Cost: ${total_monthly_cost - estimated_savings:.2f}\")\n",
    "\n",
    "print(\"\\nâœ… Cost analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3a6703",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "1. **âœ… Model Deployment**: Successfully deployed models to Vertex AI endpoints\n",
    "2. **âœ… Traffic Management**: Implemented A/B testing with traffic splitting\n",
    "3. **âœ… Health Monitoring**: Set up comprehensive health checks and monitoring\n",
    "4. **âœ… Performance Tracking**: Implemented performance metrics collection\n",
    "5. **âœ… Automated Pipeline**: Created automated deployment pipeline configuration\n",
    "6. **âœ… Cost Analysis**: Analyzed deployment costs and optimization opportunities\n",
    "\n",
    "### Key Features Demonstrated\n",
    "\n",
    "- **Endpoint Management**: Creation, deployment, and management of Vertex AI endpoints\n",
    "- **Model Serving**: Real-time prediction serving with load balancing\n",
    "- **A/B Testing**: Champion/challenger model comparison\n",
    "- **Monitoring**: Health checks, performance metrics, and alerting\n",
    "- **Cost Optimization**: Cost analysis and optimization strategies\n",
    "\n",
    "### Production Readiness Checklist\n",
    "\n",
    "- [ ] **Configure Google Cloud credentials**\n",
    "- [ ] **Set up proper IAM permissions**\n",
    "- [ ] **Configure monitoring and alerting**\n",
    "- [ ] **Implement automated rollback procedures**\n",
    "- [ ] **Set up CI/CD pipeline integration**\n",
    "- [ ] **Configure auto-scaling policies**\n",
    "- [ ] **Implement security scanning**\n",
    "- [ ] **Set up disaster recovery procedures**\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Complete the MLOps Pipeline**: Integrate with training and data pipelines\n",
    "2. **Implement Advanced Monitoring**: Set up custom dashboards and alerts\n",
    "3. **Optimize for Scale**: Implement auto-scaling and load optimization\n",
    "4. **Security Hardening**: Implement security best practices\n",
    "5. **Cost Optimization**: Apply cost reduction strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0509cb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ‰ Model Deployment Notebook Complete!\")\n",
    "print(\"ğŸ“š Ready for production deployment with Vertex AI\")\n",
    "print(\"ğŸš€ Next: Complete the end-to-end MLOps pipeline\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
