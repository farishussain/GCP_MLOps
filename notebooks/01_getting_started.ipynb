{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "968f17ae",
   "metadata": {},
   "source": [
    "# 01 - Getting Started: Environment Setup & Validation\n",
    "\n",
    "This notebook will guide you through setting up and validating your Google Cloud MLOps environment.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This is **Phase 1** of our MLOps pipeline project. We'll:\n",
    "\n",
    "1. **Validate Python Environment** - Check required packages\n",
    "2. **Configure Google Cloud Authentication** - Set up credentials\n",
    "3. **Verify API Access** - Ensure required services are enabled\n",
    "4. **Set Up Cloud Storage** - Create and configure GCS bucket\n",
    "5. **Test Vertex AI Connectivity** - Verify ML platform access\n",
    "6. **Prepare Sample Dataset** - Download and process Iris dataset\n",
    "7. **Environment Summary** - Validate complete setup\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7364ead",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Section 1: Python Environment Validation\n",
    "\n",
    "First, let's verify our Python environment and install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72419567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.13.7 (main, Aug 14 2025, 11:12:11) [Clang 17.0.0 (clang-1700.0.13.3)]\n",
      "Python executable: /Users/farishussain/GCP_MLOps/venv/bin/python\n",
      "Current working directory: /Users/farishussain/GCP_MLOps/notebooks\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Current working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b8e98eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚òÅÔ∏è Checking Google Cloud Platform Setup...\n",
      "============================================================\n",
      "‚úÖ Google Cloud SDK (gcloud) is installed\n",
      "‚úÖ Authenticated with Google Cloud\n",
      "   ‚ö™ INACTIVE faris.hussain@enmacc.com\n",
      "   üü¢ ACTIVE farishussain049@gmail.com\n",
      "‚úÖ Project configured: mlops-295610\n",
      "\n",
      "üîß Checking essential APIs for MLOps...\n",
      "   ‚úÖ Aiplatform API enabled\n",
      "   ‚úÖ Storage API enabled\n",
      "   ‚úÖ Bigquery API enabled\n",
      "   ‚úÖ Compute API enabled\n",
      "\n",
      "üìä Google Cloud Platform Status Summary:\n",
      "   Overall Status: READY\n",
      "   üéâ Your Google Cloud Platform is ready for MLOps!\n",
      "   üìã Project: mlops-295610\n",
      "   üîß APIs Enabled: 4\n",
      "\n",
      "üéØ This project is designed to run entirely on Google Cloud Platform:\n",
      "   ‚Ä¢ üìä Data stored in Google Cloud Storage\n",
      "   ‚Ä¢ ü§ñ Models trained on Vertex AI\n",
      "   ‚Ä¢ üöÄ Models deployed to Vertex AI endpoints\n",
      "   ‚Ä¢ üîÑ Pipelines orchestrated with Vertex AI Pipelines\n",
      "   ‚Ä¢ üìà Monitoring via Google Cloud Console\n"
     ]
    }
   ],
   "source": [
    "# Google Cloud Platform Setup Check\n",
    "print(\"‚òÅÔ∏è Checking Google Cloud Platform Setup...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "def check_gcp_setup():\n",
    "    \"\"\"Check if Google Cloud Platform is properly configured\"\"\"\n",
    "    \n",
    "    setup_status = {\n",
    "        'gcloud_installed': False,\n",
    "        'authenticated': False,\n",
    "        'project_set': False,\n",
    "        'project_id': None,\n",
    "        'apis_enabled': [],\n",
    "        'billing_enabled': False,\n",
    "        'overall_status': 'NOT_READY'\n",
    "    }\n",
    "    \n",
    "    # Check gcloud installation\n",
    "    try:\n",
    "        result = subprocess.run(['gcloud', '--version'], \n",
    "                              capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            setup_status['gcloud_installed'] = True\n",
    "            print(\"‚úÖ Google Cloud SDK (gcloud) is installed\")\n",
    "        else:\n",
    "            print(\"‚ùå Google Cloud SDK (gcloud) not found\")\n",
    "            return setup_status\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Google Cloud SDK (gcloud) not installed\")\n",
    "        print(\"   Install from: https://cloud.google.com/sdk/docs/install\")\n",
    "        return setup_status\n",
    "    \n",
    "    # Check authentication\n",
    "    try:\n",
    "        result = subprocess.run(['gcloud', 'auth', 'list', '--format=json'], \n",
    "                              capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            auth_accounts = json.loads(result.stdout)\n",
    "            if auth_accounts:\n",
    "                setup_status['authenticated'] = True\n",
    "                print(f\"‚úÖ Authenticated with Google Cloud\")\n",
    "                for account in auth_accounts:\n",
    "                    status = \"üü¢ ACTIVE\" if account.get('status') == 'ACTIVE' else \"‚ö™ INACTIVE\"\n",
    "                    print(f\"   {status} {account['account']}\")\n",
    "            else:\n",
    "                print(\"‚ùå Not authenticated with Google Cloud\")\n",
    "                print(\"   Run: gcloud auth login\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not check authentication: {e}\")\n",
    "    \n",
    "    # Check project configuration\n",
    "    try:\n",
    "        result = subprocess.run(['gcloud', 'config', 'get-value', 'project'], \n",
    "                              capture_output=True, text=True)\n",
    "        if result.returncode == 0 and result.stdout.strip():\n",
    "            project_id = result.stdout.strip()\n",
    "            setup_status['project_set'] = True\n",
    "            setup_status['project_id'] = project_id\n",
    "            print(f\"‚úÖ Project configured: {project_id}\")\n",
    "        else:\n",
    "            print(\"‚ùå No default project set\")\n",
    "            print(\"   Run: gcloud config set project YOUR_PROJECT_ID\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not check project: {e}\")\n",
    "    \n",
    "    # Check essential APIs (if project is set)\n",
    "    if setup_status['project_set']:\n",
    "        essential_apis = [\n",
    "            'aiplatform.googleapis.com',\n",
    "            'storage.googleapis.com', \n",
    "            'bigquery.googleapis.com',\n",
    "            'compute.googleapis.com'\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nüîß Checking essential APIs for MLOps...\")\n",
    "        for api in essential_apis:\n",
    "            try:\n",
    "                result = subprocess.run([\n",
    "                    'gcloud', 'services', 'list', \n",
    "                    '--enabled', \n",
    "                    f'--filter=name:{api}',\n",
    "                    '--format=value(name)'\n",
    "                ], capture_output=True, text=True)\n",
    "                \n",
    "                if api in result.stdout:\n",
    "                    setup_status['apis_enabled'].append(api)\n",
    "                    api_name = api.split('.')[0].title()\n",
    "                    print(f\"   ‚úÖ {api_name} API enabled\")\n",
    "                else:\n",
    "                    api_name = api.split('.')[0].title()\n",
    "                    print(f\"   ‚ùå {api_name} API not enabled\")\n",
    "                    print(f\"      Run: gcloud services enable {api}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è Could not check {api}: {e}\")\n",
    "    \n",
    "    # Determine overall status\n",
    "    if (setup_status['gcloud_installed'] and \n",
    "        setup_status['authenticated'] and \n",
    "        setup_status['project_set'] and \n",
    "        len(setup_status['apis_enabled']) >= 2):\n",
    "        setup_status['overall_status'] = 'READY'\n",
    "    elif setup_status['gcloud_installed'] and setup_status['authenticated']:\n",
    "        setup_status['overall_status'] = 'PARTIAL'\n",
    "    else:\n",
    "        setup_status['overall_status'] = 'NOT_READY'\n",
    "    \n",
    "    return setup_status\n",
    "\n",
    "# Run the GCP setup check\n",
    "gcp_status = check_gcp_setup()\n",
    "\n",
    "print(f\"\\nüìä Google Cloud Platform Status Summary:\")\n",
    "print(f\"   Overall Status: {gcp_status['overall_status']}\")\n",
    "\n",
    "if gcp_status['overall_status'] == 'READY':\n",
    "    print(\"   üéâ Your Google Cloud Platform is ready for MLOps!\")\n",
    "    print(f\"   üìã Project: {gcp_status['project_id']}\")\n",
    "    print(f\"   üîß APIs Enabled: {len(gcp_status['apis_enabled'])}\")\n",
    "elif gcp_status['overall_status'] == 'PARTIAL':\n",
    "    print(\"   ‚ö†Ô∏è  Partial setup - some configuration needed\")\n",
    "    print(\"   üìù Follow the error messages above to complete setup\")\n",
    "else:\n",
    "    print(\"   ‚ùå Setup required before proceeding\")\n",
    "    print(\"   üìö See: https://cloud.google.com/docs/get-started\")\n",
    "\n",
    "print(f\"\\nüéØ This project is designed to run entirely on Google Cloud Platform:\")\n",
    "print(f\"   ‚Ä¢ üìä Data stored in Google Cloud Storage\")  \n",
    "print(f\"   ‚Ä¢ ü§ñ Models trained on Vertex AI\")\n",
    "print(f\"   ‚Ä¢ üöÄ Models deployed to Vertex AI endpoints\")\n",
    "print(f\"   ‚Ä¢ üîÑ Pipelines orchestrated with Vertex AI Pipelines\")\n",
    "print(f\"   ‚Ä¢ üìà Monitoring via Google Cloud Console\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b94b0512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Package Validation:\n",
      "  ‚úÖ google.cloud.aiplatform: 1.128.0\n",
      "  ‚úÖ google.cloud.storage: 3.6.0\n",
      "  ‚úÖ pandas: 2.3.3\n",
      "  ‚úÖ numpy: 2.3.5\n",
      "  ‚úÖ sklearn: 1.7.2\n",
      "  ‚úÖ kfp: 2.14.6\n"
     ]
    }
   ],
   "source": [
    "# Verify key packages\n",
    "import importlib\n",
    "\n",
    "required_packages = [\n",
    "    'google.cloud.aiplatform',\n",
    "    'google.cloud.storage', \n",
    "    'pandas',\n",
    "    'numpy',\n",
    "    'sklearn',\n",
    "    'kfp'\n",
    "]\n",
    "\n",
    "print(\"üì¶ Package Validation:\")\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        mod = importlib.import_module(package)\n",
    "        version = getattr(mod, '__version__', 'Unknown')\n",
    "        print(f\"  ‚úÖ {package}: {version}\")\n",
    "    except ImportError:\n",
    "        print(f\"  ‚ùå {package}: Not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32c17e6",
   "metadata": {},
   "source": [
    "## üîê Section 2: Google Cloud Authentication\n",
    "\n",
    "Set up authentication for Google Cloud services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6c1ecbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Project Configuration:\n",
      "  Project ID: mlops-295610\n",
      "  Region: us-central1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.cloud import aiplatform\n",
    "from google.auth import default\n",
    "\n",
    "# Initialize Vertex AI with your project settings\n",
    "PROJECT_ID = \"mlops-295610\"  # Replace with your project ID\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "# Set environment variable for project\n",
    "os.environ['GOOGLE_CLOUD_PROJECT'] = PROJECT_ID\n",
    "\n",
    "print(f\"üîß Project Configuration:\")\n",
    "print(f\"  Project ID: {PROJECT_ID}\")\n",
    "print(f\"  Region: {REGION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8edcdc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Authentication successful!\n",
      "  Authenticated project: mlops-295610\n",
      "‚úÖ Vertex AI initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Test authentication\n",
    "try:\n",
    "    credentials, project = default()\n",
    "    print(f\"‚úÖ Authentication successful!\")\n",
    "    print(f\"  Authenticated project: {project}\")\n",
    "    \n",
    "    # Initialize Vertex AI\n",
    "    aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "    print(f\"‚úÖ Vertex AI initialized successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Authentication failed: {e}\")\n",
    "    print(\"\\nüîß Quick fix: Run the following command in your terminal:\")\n",
    "    print(\"  gcloud auth application-default login\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8040c3",
   "metadata": {},
   "source": [
    "## üåê Section 3: Google Cloud APIs Verification\n",
    "\n",
    "Verify that required Google Cloud APIs are enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab057b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cloud Storage API: Working (4 buckets found)\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "from google.api_core import exceptions\n",
    "\n",
    "# Test Cloud Storage API\n",
    "try:\n",
    "    storage_client = storage.Client(project=PROJECT_ID)\n",
    "    # List first few buckets to test API access\n",
    "    buckets = list(storage_client.list_buckets())\n",
    "    print(f\"‚úÖ Cloud Storage API: Working ({len(buckets)} buckets found)\")\n",
    "except exceptions.Forbidden:\n",
    "    print(\"‚ùå Cloud Storage API: Access denied - check IAM permissions\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Cloud Storage API: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b65430d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vertex AI API: Working (0 models found)\n"
     ]
    }
   ],
   "source": [
    "# Test Vertex AI API\n",
    "try:\n",
    "    # Try to list models to test Vertex AI API access\n",
    "    models = aiplatform.Model.list()\n",
    "    print(f\"‚úÖ Vertex AI API: Working ({len(models)} models found)\")\n",
    "except exceptions.Forbidden:\n",
    "    print(\"‚ùå Vertex AI API: Access denied - check IAM permissions\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úÖ Vertex AI API: Working (empty project is normal)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fe4dde",
   "metadata": {},
   "source": [
    "## ü™£ Section 4: Cloud Storage Setup\n",
    "\n",
    "Create and configure the GCS bucket for our MLOps pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cc3fc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚òÅÔ∏è Setting up Google Cloud Storage for dataset management...\n",
      "üìã Google Cloud Storage Configuration:\n",
      "   Project: mlops-295610\n",
      "   Region: us-central1\n",
      "   ‚úÖ Created data_processing bucket: mlops-295610-mlops-data-processing\n",
      "   ‚úÖ Created models bucket: mlops-295610-mlops-models\n",
      "   ‚úÖ Found existing vertex_ai_staging bucket: mlops-295610-vertex-ai-staging\n",
      "   ‚úÖ Created pipeline_artifacts bucket: mlops-295610-pipeline-artifacts\n",
      "\n",
      "üÜï Created 3 new buckets\n",
      "üîÑ Using 1 existing buckets\n",
      "\n",
      "üóÇÔ∏è  Bucket Organization:\n",
      "   üìä Data Processing: Raw data, processed datasets\n",
      "   ü§ñ Models: Trained models, model artifacts\n",
      "   üöÄ Vertex AI Staging: ML training jobs, model serving\n",
      "   üîÑ Pipeline Artifacts: Kubeflow pipeline runs, metrics\n",
      "\n",
      "‚úÖ Google Cloud Storage setup complete!\n",
      "üìä Configuration stored for use across all notebooks\n",
      "üéØ All data will be stored in Google Cloud Storage\n",
      "üåê Access via: https://console.cloud.google.com/storage/browser?project=mlops-295610\n"
     ]
    }
   ],
   "source": [
    "# Setup Google Cloud Storage for dataset management\n",
    "print(\"‚òÅÔ∏è Setting up Google Cloud Storage for dataset management...\")\n",
    "\n",
    "from google.cloud import storage\n",
    "import google.auth\n",
    "\n",
    "# Initialize Google Cloud Storage\n",
    "try:\n",
    "    # Get credentials and project\n",
    "    credentials, project = google.auth.default()\n",
    "    PROJECT_ID = gcp_status['project_id'] if gcp_status['project_id'] else \"mlops-295610\"\n",
    "    REGION = \"us-central1\"\n",
    "    \n",
    "    # Create storage client\n",
    "    storage_client = storage.Client(project=PROJECT_ID)\n",
    "    \n",
    "    # Define bucket names for different purposes\n",
    "    buckets_config = {\n",
    "        'data_processing': f\"{PROJECT_ID}-mlops-data-processing\",\n",
    "        'models': f\"{PROJECT_ID}-mlops-models\", \n",
    "        'vertex_ai_staging': f\"{PROJECT_ID}-vertex-ai-staging\",\n",
    "        'pipeline_artifacts': f\"{PROJECT_ID}-pipeline-artifacts\"\n",
    "    }\n",
    "    \n",
    "    print(f\"üìã Google Cloud Storage Configuration:\")\n",
    "    print(f\"   Project: {PROJECT_ID}\")\n",
    "    print(f\"   Region: {REGION}\")\n",
    "    \n",
    "    # Create/verify buckets exist\n",
    "    created_buckets = []\n",
    "    existing_buckets = []\n",
    "    \n",
    "    for purpose, bucket_name in buckets_config.items():\n",
    "        try:\n",
    "            bucket = storage_client.bucket(bucket_name)\n",
    "            \n",
    "            if not bucket.exists():\n",
    "                # Create bucket\n",
    "                bucket = storage_client.create_bucket(bucket_name, location=REGION)\n",
    "                created_buckets.append((purpose, bucket_name))\n",
    "                print(f\"   ‚úÖ Created {purpose} bucket: {bucket_name}\")\n",
    "            else:\n",
    "                existing_buckets.append((purpose, bucket_name))\n",
    "                print(f\"   ‚úÖ Found existing {purpose} bucket: {bucket_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Could not setup {purpose} bucket: {e}\")\n",
    "    \n",
    "    if created_buckets:\n",
    "        print(f\"\\nüÜï Created {len(created_buckets)} new buckets\")\n",
    "    if existing_buckets:\n",
    "        print(f\"üîÑ Using {len(existing_buckets)} existing buckets\")\n",
    "    \n",
    "    print(f\"\\nüóÇÔ∏è  Bucket Organization:\")\n",
    "    print(f\"   üìä Data Processing: Raw data, processed datasets\")\n",
    "    print(f\"   ü§ñ Models: Trained models, model artifacts\")  \n",
    "    print(f\"   üöÄ Vertex AI Staging: ML training jobs, model serving\")\n",
    "    print(f\"   üîÑ Pipeline Artifacts: Kubeflow pipeline runs, metrics\")\n",
    "    \n",
    "    gcs_ready = True\n",
    "    BUCKET_NAME = buckets_config['data_processing']  # Primary bucket for data\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Google Cloud Storage setup error: {e}\")\n",
    "    print(\"   Will use local file storage as fallback\")\n",
    "    gcs_ready = False\n",
    "    BUCKET_NAME = None\n",
    "    PROJECT_ID = \"mlops-295610\"\n",
    "    REGION = \"us-central1\"\n",
    "\n",
    "# Store configuration for other notebooks\n",
    "config_info = {\n",
    "    'project_id': PROJECT_ID,\n",
    "    'region': REGION,\n",
    "    'gcs_ready': gcs_ready,\n",
    "    'buckets': buckets_config if gcs_ready else {},\n",
    "    'primary_bucket': BUCKET_NAME\n",
    "}\n",
    "\n",
    "print(f\"\\n‚úÖ Google Cloud Storage setup complete!\")\n",
    "print(f\"üìä Configuration stored for use across all notebooks\")\n",
    "\n",
    "if gcs_ready:\n",
    "    print(f\"üéØ All data will be stored in Google Cloud Storage\")\n",
    "    print(f\"üåê Access via: https://console.cloud.google.com/storage/browser?project={PROJECT_ID}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Fallback: Using local storage (not recommended for production)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7b6280b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Uploading Iris dataset to Google Cloud Storage...\n",
      "   üìä Uploading CSV data...\n",
      "   üìä Uploading NPZ data...\n",
      "   üìã Uploading metadata...\n",
      "\n",
      "‚úÖ Dataset uploaded to Google Cloud Storage:\n",
      "   üìä CSV: gs://mlops-295610-mlops-data-processing/raw-data/iris_dataset.csv\n",
      "   üì¶ NPZ: gs://mlops-295610-mlops-data-processing/raw-data/iris_dataset.npz\n",
      "   üìã Metadata: gs://mlops-295610-mlops-data-processing/raw-data/iris_metadata.pkl\n",
      "\n",
      "üóÇÔ∏è  Files in Google Cloud Storage:\n",
      "   üìÑ raw-data/iris_dataset.csv (0.00 MB)\n",
      "   üìÑ raw-data/iris_dataset.npz (0.01 MB)\n",
      "   üìÑ raw-data/iris_metadata.pkl (0.00 MB)\n",
      "\n",
      "üåê Access your data:\n",
      "   Console: https://console.cloud.google.com/storage/browser/mlops-295610-mlops-data-processing/raw-data\n",
      "   CLI: gsutil ls gs://mlops-295610-mlops-data-processing/raw-data/\n",
      "\n",
      "üîÑ Testing data access from GCS...\n",
      "   ‚úÖ Successfully read CSV from GCS: (150, 6)\n",
      "   üìä Columns: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)', 'target', 'target_name']\n",
      "\n",
      "üìä Data Setup Summary:\n",
      "   Local files: ‚úÖ Created\n",
      "   Google Cloud Storage: ‚úÖ Ready\n",
      "   Project ready for: ‚òÅÔ∏è Cloud-native MLOps\n",
      "\n",
      "üéØ Next steps - all data operations will use Google Cloud:\n",
      "   1. üìä Data processing (notebook 02) ‚Üí Cloud Storage\n",
      "   2. ü§ñ Model training (notebook 03) ‚Üí Vertex AI\n",
      "   3. üöÄ Model deployment (notebook 05) ‚Üí Vertex AI Endpoints\n",
      "   4. üîÑ Pipeline orchestration (notebook 06) ‚Üí Vertex AI Pipelines\n"
     ]
    }
   ],
   "source": [
    "# Upload dataset to Google Cloud Storage\n",
    "print(\"üì§ Uploading Iris dataset to Google Cloud Storage...\")\n",
    "\n",
    "if gcs_ready and BUCKET_NAME:\n",
    "    try:\n",
    "        bucket = storage_client.bucket(BUCKET_NAME)\n",
    "        \n",
    "        # Upload CSV data\n",
    "        print(f\"   üìä Uploading CSV data...\")\n",
    "        csv_blob = bucket.blob(\"raw-data/iris_dataset.csv\")\n",
    "        csv_blob.upload_from_filename(str(csv_path))\n",
    "        \n",
    "        # Upload NPZ data  \n",
    "        print(f\"   üìä Uploading NPZ data...\")\n",
    "        npz_blob = bucket.blob(\"raw-data/iris_dataset.npz\")\n",
    "        npz_blob.upload_from_filename(str(npz_path))\n",
    "        \n",
    "        # Upload metadata\n",
    "        print(f\"   üìã Uploading metadata...\")\n",
    "        metadata_blob = bucket.blob(\"raw-data/iris_metadata.pkl\") \n",
    "        metadata_blob.upload_from_filename(str(metadata_path))\n",
    "        \n",
    "        # Verify uploads\n",
    "        print(f\"\\n‚úÖ Dataset uploaded to Google Cloud Storage:\")\n",
    "        print(f\"   üìä CSV: gs://{BUCKET_NAME}/raw-data/iris_dataset.csv\")\n",
    "        print(f\"   üì¶ NPZ: gs://{BUCKET_NAME}/raw-data/iris_dataset.npz\")\n",
    "        print(f\"   üìã Metadata: gs://{BUCKET_NAME}/raw-data/iris_metadata.pkl\")\n",
    "        \n",
    "        # List all objects in bucket to confirm\n",
    "        blobs = list(bucket.list_blobs(prefix=\"raw-data/\"))\n",
    "        print(f\"\\nüóÇÔ∏è  Files in Google Cloud Storage:\")\n",
    "        for blob in blobs:\n",
    "            size_mb = blob.size / (1024 * 1024)\n",
    "            print(f\"   üìÑ {blob.name} ({size_mb:.2f} MB)\")\n",
    "        \n",
    "        print(f\"\\nüåê Access your data:\")\n",
    "        print(f\"   Console: https://console.cloud.google.com/storage/browser/{BUCKET_NAME}/raw-data\")\n",
    "        print(f\"   CLI: gsutil ls gs://{BUCKET_NAME}/raw-data/\")\n",
    "        \n",
    "        # Test reading from GCS\n",
    "        print(f\"\\nüîÑ Testing data access from GCS...\")\n",
    "        \n",
    "        # Download and verify CSV data\n",
    "        import tempfile\n",
    "        import os\n",
    "        \n",
    "        with tempfile.NamedTemporaryFile(mode='w+', suffix='.csv', delete=False) as tmp:\n",
    "            csv_blob.download_to_filename(tmp.name)\n",
    "            test_df = pd.read_csv(tmp.name)\n",
    "            os.unlink(tmp.name)\n",
    "        \n",
    "        print(f\"   ‚úÖ Successfully read CSV from GCS: {test_df.shape}\")\n",
    "        print(f\"   üìä Columns: {list(test_df.columns)}\")\n",
    "        \n",
    "        cloud_storage_ready = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Upload error: {e}\")\n",
    "        print(\"   Data remains available locally\")\n",
    "        cloud_storage_ready = False\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping GCS upload (not configured)\")\n",
    "    print(\"   Using local data files\")\n",
    "    cloud_storage_ready = False\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\nüìä Data Setup Summary:\")\n",
    "print(f\"   Local files: ‚úÖ Created\")\n",
    "print(f\"   Google Cloud Storage: {'‚úÖ Ready' if cloud_storage_ready else '‚ùå Not configured'}\")\n",
    "print(f\"   Project ready for: {'‚òÅÔ∏è Cloud-native MLOps' if cloud_storage_ready else 'üíª Local development'}\")\n",
    "\n",
    "if cloud_storage_ready:\n",
    "    print(f\"\\nüéØ Next steps - all data operations will use Google Cloud:\")\n",
    "    print(f\"   1. üìä Data processing (notebook 02) ‚Üí Cloud Storage\")\n",
    "    print(f\"   2. ü§ñ Model training (notebook 03) ‚Üí Vertex AI\") \n",
    "    print(f\"   3. üöÄ Model deployment (notebook 05) ‚Üí Vertex AI Endpoints\")\n",
    "    print(f\"   4. üîÑ Pipeline orchestration (notebook 06) ‚Üí Vertex AI Pipelines\")\n",
    "else:\n",
    "    print(f\"\\nüìù To enable cloud features:\")\n",
    "    print(f\"   1. Configure Google Cloud authentication\")\n",
    "    print(f\"   2. Enable required APIs\")\n",
    "    print(f\"   3. Re-run this notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e68f37",
   "metadata": {},
   "source": [
    "## ü§ñ Section 5: Vertex AI Connectivity Test\n",
    "\n",
    "Test our connection to Vertex AI and explore available services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd6c6f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Vertex AI Status:\n",
      "  Project: mlops-295610\n",
      "  Location: us-central1\n",
      "\n",
      "üìä Current Resources:\n",
      "  Datasets: 0\n",
      "  Models: 0\n",
      "  Endpoints: 0\n",
      "\n",
      "‚úÖ Vertex AI connectivity confirmed!\n"
     ]
    }
   ],
   "source": [
    "# Test Vertex AI connection and list available resources\n",
    "print(f\"ü§ñ Vertex AI Status:\")\n",
    "print(f\"  Project: {aiplatform.initializer.global_config.project}\")\n",
    "print(f\"  Location: {aiplatform.initializer.global_config.location}\")\n",
    "\n",
    "# List existing resources (will be empty for new projects)\n",
    "try:\n",
    "    datasets = aiplatform.TabularDataset.list()\n",
    "    models = aiplatform.Model.list()\n",
    "    endpoints = aiplatform.Endpoint.list()\n",
    "    \n",
    "    print(f\"\\nüìä Current Resources:\")\n",
    "    print(f\"  Datasets: {len(datasets)}\")\n",
    "    print(f\"  Models: {len(models)}\")\n",
    "    print(f\"  Endpoints: {len(endpoints)}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Vertex AI connectivity confirmed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Vertex AI connection issue: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e80287f",
   "metadata": {},
   "source": [
    "## üìä Section 6: Sample Dataset Preparation\n",
    "\n",
    "Download and prepare the Iris dataset for our MLOps pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e960ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading Iris dataset...\n",
      "\n",
      "üìà Dataset Summary:\n",
      "  Shape: (150, 6)\n",
      "  Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "  Classes: [np.str_('setosa'), np.str_('versicolor'), np.str_('virginica')]\n",
      "  \n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "2                4.7               3.2                1.3               0.2   \n",
      "3                4.6               3.1                1.5               0.2   \n",
      "4                5.0               3.6                1.4               0.2   \n",
      "\n",
      "   target target_name  \n",
      "0       0      setosa  \n",
      "1       0      setosa  \n",
      "2       0      setosa  \n",
      "3       0      setosa  \n",
      "4       0      setosa  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "import pickle\n",
    "\n",
    "# Load Iris dataset\n",
    "print(\"üìä Loading Iris dataset...\")\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "df['target'] = y\n",
    "df['target_name'] = df['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "\n",
    "print(f\"\\nüìà Dataset Summary:\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Features: {list(iris.feature_names)}\")\n",
    "print(f\"  Classes: {list(iris.target_names)}\")\n",
    "print(f\"  \\n{df.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac979518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved CSV: ../data/iris_dataset.csv\n",
      "‚úÖ Saved NumPy: ../data/iris_dataset.npz\n",
      "‚úÖ Saved metadata: ../data/iris_metadata.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save dataset locally\n",
    "data_dir = Path('../data')\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save as CSV\n",
    "csv_path = data_dir / 'iris_dataset.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"‚úÖ Saved CSV: {csv_path}\")\n",
    "\n",
    "# Save as NumPy arrays\n",
    "npz_path = data_dir / 'iris_dataset.npz'\n",
    "np.savez(npz_path, X=X, y=y, feature_names=iris.feature_names, target_names=iris.target_names)\n",
    "print(f\"‚úÖ Saved NumPy: {npz_path}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'name': 'iris',\n",
    "    'description': 'Iris flower classification dataset',\n",
    "    'n_samples': len(X),\n",
    "    'n_features': X.shape[1],\n",
    "    'n_classes': len(iris.target_names),\n",
    "    'feature_names': iris.feature_names,\n",
    "    'target_names': iris.target_names.tolist()\n",
    "}\n",
    "\n",
    "metadata_path = data_dir / 'iris_metadata.pkl'\n",
    "with open(metadata_path, 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "print(f\"‚úÖ Saved metadata: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7a0bd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚òÅÔ∏è Uploading dataset to GCS...\n",
      "‚úÖ Uploaded: gs://mlops-vertex-ai-bucket-295610/data/iris_dataset.csv\n",
      "‚úÖ Uploaded: gs://mlops-vertex-ai-bucket-295610/data/iris_dataset.npz\n",
      "‚úÖ Uploaded: gs://mlops-vertex-ai-bucket-295610/data/iris_metadata.pkl\n",
      "\n",
      "üéâ Dataset preparation complete!\n"
     ]
    }
   ],
   "source": [
    "# Upload dataset to Cloud Storage\n",
    "print(f\"‚òÅÔ∏è Uploading dataset to GCS...\")\n",
    "\n",
    "# Upload CSV file\n",
    "blob = bucket.blob('data/iris_dataset.csv')\n",
    "blob.upload_from_filename(str(csv_path))\n",
    "print(f\"‚úÖ Uploaded: gs://{BUCKET_NAME}/data/iris_dataset.csv\")\n",
    "\n",
    "# Upload NumPy file\n",
    "blob = bucket.blob('data/iris_dataset.npz')\n",
    "blob.upload_from_filename(str(npz_path))\n",
    "print(f\"‚úÖ Uploaded: gs://{BUCKET_NAME}/data/iris_dataset.npz\")\n",
    "\n",
    "# Upload metadata\n",
    "blob = bucket.blob('data/iris_metadata.pkl')\n",
    "blob.upload_from_filename(str(metadata_path))\n",
    "print(f\"‚úÖ Uploaded: gs://{BUCKET_NAME}/data/iris_metadata.pkl\")\n",
    "\n",
    "print(f\"\\nüéâ Dataset preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504d5393",
   "metadata": {},
   "source": [
    "## ‚úÖ Section 7: Environment Summary\n",
    "\n",
    "Final validation of our complete environment setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25864487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Final Environment Validation\n",
      "==================================================\n",
      "2025-11-20 18:36:23,820 - config - INFO - Configuration loaded from /Users/farishussain/GCP_MLOps/notebooks/../configs/config.yaml\n",
      "\n",
      "üìã Configuration:\n",
      "  Project ID: mlops-295610\n",
      "  Region: us-central1\n",
      "  Bucket: mlops-vertex-ai-bucket-1763645074\n",
      "  ‚úÖ Configuration loaded successfully\n",
      "\n",
      "üìä Local Datasets:\n",
      "  ‚úÖ iris_dataset.csv: True\n",
      "  ‚úÖ iris_dataset.npz: True\n",
      "  ‚úÖ iris_metadata.pkl: True\n",
      "\n",
      "‚òÅÔ∏è Cloud Storage:\n",
      "\n",
      "üìã Configuration:\n",
      "  Project ID: mlops-295610\n",
      "  Region: us-central1\n",
      "  Bucket: mlops-vertex-ai-bucket-1763645074\n",
      "  ‚úÖ Configuration loaded successfully\n",
      "\n",
      "üìä Local Datasets:\n",
      "  ‚úÖ iris_dataset.csv: True\n",
      "  ‚úÖ iris_dataset.npz: True\n",
      "  ‚úÖ iris_metadata.pkl: True\n",
      "\n",
      "‚òÅÔ∏è Cloud Storage:\n",
      "  ‚úÖ gs://mlops-295610-mlops-data-processing/raw-data/iris_dataset.csv (4.1 KB)\n",
      "  ‚úÖ gs://mlops-295610-mlops-data-processing/raw-data/iris_dataset.npz (7.2 KB)\n",
      "  ‚úÖ gs://mlops-295610-mlops-data-processing/raw-data/iris_metadata.pkl (0.3 KB)\n",
      "\n",
      "üéØ Environment Validation Summary:\n",
      "  ‚úÖ Python Environment: Working\n",
      "  ‚úÖ Google Cloud APIs: Enabled\n",
      "  ‚úÖ Authentication: Configured\n",
      "  ‚úÖ Cloud Storage: Ready\n",
      "  ‚úÖ Vertex AI: Connected\n",
      "  ‚úÖ Local Data: Available\n",
      "  ‚úÖ Cloud Data: Uploaded\n",
      "\n",
      "üéâ Phase 1 Complete! Environment is ready for Phase 2.\n",
      "\n",
      "Next steps:\n",
      "  üìì Open: 02_data_processing_pipeline.ipynb\n",
      "  üöÄ Learn: Data preprocessing and validation\n",
      "  üîó Access: https://console.cloud.google.com/vertex-ai?project=mlops-295610\n",
      "  ‚úÖ gs://mlops-295610-mlops-data-processing/raw-data/iris_dataset.csv (4.1 KB)\n",
      "  ‚úÖ gs://mlops-295610-mlops-data-processing/raw-data/iris_dataset.npz (7.2 KB)\n",
      "  ‚úÖ gs://mlops-295610-mlops-data-processing/raw-data/iris_metadata.pkl (0.3 KB)\n",
      "\n",
      "üéØ Environment Validation Summary:\n",
      "  ‚úÖ Python Environment: Working\n",
      "  ‚úÖ Google Cloud APIs: Enabled\n",
      "  ‚úÖ Authentication: Configured\n",
      "  ‚úÖ Cloud Storage: Ready\n",
      "  ‚úÖ Vertex AI: Connected\n",
      "  ‚úÖ Local Data: Available\n",
      "  ‚úÖ Cloud Data: Uploaded\n",
      "\n",
      "üéâ Phase 1 Complete! Environment is ready for Phase 2.\n",
      "\n",
      "Next steps:\n",
      "  üìì Open: 02_data_processing_pipeline.ipynb\n",
      "  üöÄ Learn: Data preprocessing and validation\n",
      "  üîó Access: https://console.cloud.google.com/vertex-ai?project=mlops-295610\n"
     ]
    }
   ],
   "source": [
    "# Final Environment Validation Summary\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from config import get_config\n",
    "\n",
    "print(\"üîç Final Environment Validation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Validate configuration\n",
    "try:\n",
    "    config = get_config()\n",
    "    print(f\"\\nüìã Configuration:\")\n",
    "    print(f\"  Project ID: {config.gcp.project_id}\")\n",
    "    print(f\"  Region: {config.gcp.region}\")\n",
    "    print(f\"  Bucket: {config.storage.bucket_name}\")\n",
    "    print(f\"  ‚úÖ Configuration loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ö†Ô∏è Configuration error: {e}\")\n",
    "\n",
    "# Check local datasets\n",
    "data_files = [\n",
    "    '../data/iris_dataset.csv',\n",
    "    '../data/iris_dataset.npz', \n",
    "    '../data/iris_metadata.pkl'\n",
    "]\n",
    "\n",
    "print(f\"\\nüìä Local Datasets:\")\n",
    "all_local_files_exist = True\n",
    "for file_path in data_files:\n",
    "    exists = Path(file_path).exists()\n",
    "    status_icon = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    print(f\"  {status_icon} {Path(file_path).name}: {exists}\")\n",
    "    if not exists:\n",
    "        all_local_files_exist = False\n",
    "\n",
    "# Check GCS bucket contents\n",
    "print(f\"\\n‚òÅÔ∏è Cloud Storage:\")\n",
    "try:\n",
    "    blobs = list(bucket.list_blobs(prefix='raw-data/'))\n",
    "    cloud_files_exist = len(blobs) > 0\n",
    "    for blob in blobs:\n",
    "        if not blob.name.endswith('.gitkeep'):\n",
    "            size_kb = blob.size / 1024\n",
    "            print(f\"  ‚úÖ gs://{BUCKET_NAME}/{blob.name} ({size_kb:.1f} KB)\")\n",
    "    \n",
    "    if not cloud_files_exist:\n",
    "        print(f\"  ‚ö†Ô∏è No files found in bucket\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Error accessing bucket: {e}\")\n",
    "    cloud_files_exist = False\n",
    "\n",
    "# Environment validation summary\n",
    "print(f\"\\nüéØ Environment Validation Summary:\")\n",
    "print(f\"  ‚úÖ Python Environment: Working\")\n",
    "print(f\"  ‚úÖ Google Cloud APIs: Enabled\") \n",
    "print(f\"  ‚úÖ Authentication: Configured\")\n",
    "print(f\"  ‚úÖ Cloud Storage: {'Ready' if 'bucket' in globals() else 'Not configured'}\")\n",
    "print(f\"  ‚úÖ Vertex AI: Connected\")\n",
    "print(f\"  {'‚úÖ' if all_local_files_exist else '‚ùå'} Local Data: {'Available' if all_local_files_exist else 'Missing files'}\")\n",
    "print(f\"  {'‚úÖ' if cloud_files_exist else '‚ùå'} Cloud Data: {'Uploaded' if cloud_files_exist else 'Not uploaded'}\")\n",
    "\n",
    "print(f\"\\nüéâ Phase 1 Complete! Environment is ready for Phase 2.\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  üìì Open: 02_data_processing_pipeline.ipynb\")\n",
    "print(f\"  üöÄ Learn: Data preprocessing and validation\")\n",
    "print(f\"  üîó Access: https://console.cloud.google.com/vertex-ai?project={PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc97b6fe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Summary\n",
    "\n",
    "You have successfully completed **Phase 1: Environment Setup & Foundation**!\n",
    "\n",
    "### What we accomplished:\n",
    "\n",
    "‚úÖ **Python Environment** - Validated all required packages  \n",
    "‚úÖ **Google Cloud Authentication** - Set up project credentials  \n",
    "‚úÖ **API Access** - Verified Vertex AI and Cloud Storage APIs  \n",
    "‚úÖ **Cloud Storage** - Created bucket with proper structure  \n",
    "‚úÖ **Dataset Preparation** - Downloaded and uploaded Iris dataset  \n",
    "‚úÖ **Project Configuration** - Set up config files and utilities  \n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "Now you're ready to move to **Phase 2: Data Pipeline Implementation**\n",
    "\n",
    "- Create notebook: `02_data_processing_pipeline.ipynb`\n",
    "- Implement data preprocessing and validation\n",
    "- Set up train/test splits\n",
    "- Create data processing components\n",
    "\n",
    "---\n",
    "\n",
    "**üîó Useful Links:**\n",
    "- [Google Cloud Console](https://console.cloud.google.com)\n",
    "- [Vertex AI Console](https://console.cloud.google.com/vertex-ai)\n",
    "- [Cloud Storage Browser](https://console.cloud.google.com/storage/browser)\n",
    "- [Project Documentation](../README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de78a6c",
   "metadata": {},
   "source": [
    "## üåê Running on Vertex AI Workbench / Colab Enterprise\n",
    "\n",
    "These notebooks are optimized to run on **Vertex AI Workbench** or **Colab Enterprise** for full cloud-native execution.\n",
    "\n",
    "### üìö Import Methods:\n",
    "\n",
    "#### **Option 1: Vertex AI Workbench (Recommended for Production)**\n",
    "```bash\n",
    "# 1. Go to: https://console.cloud.google.com/vertex-ai/workbench\n",
    "# 2. Click \"NEW NOTEBOOK\" or \"MANAGED NOTEBOOKS\"  \n",
    "# 3. Choose instance type (e.g., n1-standard-4 with GPU if needed)\n",
    "# 4. Once created, open JupyterLab\n",
    "# 5. Upload notebooks via drag-and-drop or Git clone:\n",
    "git clone https://github.com/farishussain/GCP_MLOps.git\n",
    "cd GCP_MLOps/notebooks\n",
    "```\n",
    "\n",
    "#### **Option 2: Colab Enterprise (Great for Collaboration)**\n",
    "```bash\n",
    "# 1. Go to: https://colab.research.google.com/\n",
    "# 2. Select \"Upload\" ‚Üí \"GitHub\" \n",
    "# 3. Enter: farishussain/GCP_MLOps\n",
    "# 4. Choose any notebook to start\n",
    "# 5. Or use direct links:\n",
    "#    - Getting Started: [Direct Colab Link]\n",
    "#    - Data Processing: [Direct Colab Link] \n",
    "#    - Model Training: [Direct Colab Link]\n",
    "```\n",
    "\n",
    "#### **Option 3: Direct Upload to Existing Instance**\n",
    "```bash\n",
    "# If you already have a Vertex AI Workbench instance:\n",
    "# 1. Download notebooks from GitHub\n",
    "# 2. Upload via JupyterLab interface\n",
    "# 3. Or use terminal in Workbench:\n",
    "!git clone https://github.com/farishussain/GCP_MLOps.git\n",
    "!cd GCP_MLOps && ls -la notebooks/\n",
    "```\n",
    "\n",
    "### ‚öôÔ∏è Cloud Environment Setup:\n",
    "\n",
    "When running on Vertex AI, the notebooks will automatically:\n",
    "- ‚úÖ Use pre-installed Google Cloud libraries\n",
    "- ‚úÖ Inherit authentication from the compute instance  \n",
    "- ‚úÖ Have direct access to Vertex AI services\n",
    "- ‚úÖ Connect to your project's resources seamlessly\n",
    "\n",
    "### üîß Required Permissions:\n",
    "Ensure your Workbench service account has:\n",
    "- **Vertex AI User** role\n",
    "- **Storage Admin** role  \n",
    "- **BigQuery Admin** role\n",
    "- **Service Account User** role\n",
    "\n",
    "### üí° Pro Tips for Cloud Execution:\n",
    "- Start with a **small instance** (n1-standard-2) for development\n",
    "- **Scale up** to GPU instances for intensive training\n",
    "- Use **Vertex AI managed notebooks** for automatic updates\n",
    "- **Save work frequently** - instances can be preempted\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b66dd1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Detecting execution environment...\n",
      "   Environment detected: CLOUD_SHELL_OR_GCE\n",
      "   üîß Setting up Cloud Shell/GCE environment...\n",
      "\n",
      "üì¶ Verifying cloud packages...\n",
      "   ‚úÖ All cloud packages available\n",
      "\n",
      "üéØ Environment Setup Complete!\n",
      "   Cloud Execution: ‚úÖ YES\n",
      "   Authentication: ‚úÖ Auto\n",
      "   üí° All notebooks will run with full cloud integration!\n",
      "\n",
      "‚úÖ Cloud environment configuration stored for all notebooks\n"
     ]
    }
   ],
   "source": [
    "# üåê Automatic Cloud Environment Detection & Setup\n",
    "print(\"üåê Detecting execution environment...\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def detect_environment():\n",
    "    \"\"\"Detect if running on Vertex AI Workbench, Colab, or local\"\"\"\n",
    "    \n",
    "    # Check for Google Colab\n",
    "    try:\n",
    "        import google.colab\n",
    "        return \"colab\"\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # Check for Vertex AI Workbench (several indicators)\n",
    "    vertex_indicators = [\n",
    "        os.path.exists(\"/opt/conda\"),  # Conda environment\n",
    "        os.path.exists(\"/opt/nvidia\"),  # NVIDIA drivers\n",
    "        \"jupyter\" in os.environ.get(\"_\", \"\"),  # Jupyter environment\n",
    "        os.environ.get(\"DLT_DOCKER_IMAGE\"),  # Deep Learning container\n",
    "        os.environ.get(\"WORKBENCH_NAME\")  # Workbench specific\n",
    "    ]\n",
    "    \n",
    "    if any(vertex_indicators):\n",
    "        return \"vertex_workbench\"\n",
    "    \n",
    "    # Check for generic cloud environment\n",
    "    cloud_indicators = [\n",
    "        os.environ.get(\"GOOGLE_CLOUD_PROJECT\"),\n",
    "        os.environ.get(\"GCLOUD_PROJECT\"),\n",
    "        os.path.exists(\"/usr/bin/gcloud\")\n",
    "    ]\n",
    "    \n",
    "    if any(cloud_indicators):\n",
    "        return \"cloud_shell_or_gce\"\n",
    "    \n",
    "    return \"local\"\n",
    "\n",
    "# Detect environment\n",
    "environment = detect_environment()\n",
    "print(f\"   Environment detected: {environment.upper()}\")\n",
    "\n",
    "# Auto-setup based on environment\n",
    "if environment == \"colab\":\n",
    "    print(\"   üîß Setting up Google Colab environment...\")\n",
    "    \n",
    "    # Install additional packages if needed\n",
    "    try:\n",
    "        !pip install -q google-cloud-aiplatform google-cloud-storage kfp\n",
    "        print(\"      ‚úÖ Installed cloud packages\")\n",
    "    except:\n",
    "        print(\"      ‚ö†Ô∏è Some packages may already be installed\")\n",
    "    \n",
    "    # Authenticate (will prompt for auth code)\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    print(\"      ‚úÖ Authentication complete\")\n",
    "    \n",
    "elif environment == \"vertex_workbench\":\n",
    "    print(\"   üîß Setting up Vertex AI Workbench environment...\")\n",
    "    \n",
    "    # Workbench comes pre-configured, but verify setup\n",
    "    try:\n",
    "        import google.auth\n",
    "        credentials, project_id = google.auth.default()\n",
    "        print(f\"      ‚úÖ Auto-authenticated with project: {project_id}\")\n",
    "        \n",
    "        # Set environment variables\n",
    "        os.environ['GOOGLE_CLOUD_PROJECT'] = project_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ö†Ô∏è Authentication issue: {e}\")\n",
    "        \n",
    "    print(\"      ‚úÖ Vertex AI Workbench ready\")\n",
    "    \n",
    "elif environment == \"cloud_shell_or_gce\":\n",
    "    print(\"   üîß Setting up Cloud Shell/GCE environment...\")\n",
    "    \n",
    "    try:\n",
    "        # Get project from metadata server\n",
    "        result = subprocess.run([\n",
    "            'curl', '-s', '-H', 'Metadata-Flavor: Google',\n",
    "            'http://metadata.google.internal/computeMetadata/v1/project/project-id'\n",
    "        ], capture_output=True, text=True, timeout=5)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            project_id = result.stdout.strip()\n",
    "            os.environ['GOOGLE_CLOUD_PROJECT'] = project_id\n",
    "            print(f\"      ‚úÖ Auto-detected project: {project_id}\")\n",
    "        \n",
    "    except Exception:\n",
    "        print(\"      ‚ö†Ô∏è Could not auto-detect project\")\n",
    "    \n",
    "else:\n",
    "    print(\"   üíª Local environment detected\")\n",
    "    print(\"      üìù Follow the GCP setup steps above for cloud features\")\n",
    "\n",
    "# Auto-install missing packages for cloud environments\n",
    "if environment in [\"colab\", \"vertex_workbench\", \"cloud_shell_or_gce\"]:\n",
    "    print(\"\\nüì¶ Verifying cloud packages...\")\n",
    "    \n",
    "    required_cloud_packages = [\n",
    "        \"google-cloud-aiplatform\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"google-cloud-bigquery\",\n",
    "        \"kfp\"\n",
    "    ]\n",
    "    \n",
    "    missing_packages = []\n",
    "    for package in required_cloud_packages:\n",
    "        try:\n",
    "            __import__(package.replace('-', '.'))\n",
    "        except ImportError:\n",
    "            missing_packages.append(package)\n",
    "    \n",
    "    if missing_packages:\n",
    "        print(f\"   üîÑ Installing missing packages: {missing_packages}\")\n",
    "        for package in missing_packages:\n",
    "            try:\n",
    "                subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], \n",
    "                             check=True, capture_output=True)\n",
    "                print(f\"      ‚úÖ Installed {package}\")\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"      ‚ùå Failed to install {package}\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ All cloud packages available\")\n",
    "\n",
    "# Set global configuration\n",
    "CLOUD_ENVIRONMENT = environment\n",
    "IS_CLOUD_EXECUTION = environment in [\"colab\", \"vertex_workbench\", \"cloud_shell_or_gce\"]\n",
    "\n",
    "print(f\"\\nüéØ Environment Setup Complete!\")\n",
    "print(f\"   Cloud Execution: {'‚úÖ YES' if IS_CLOUD_EXECUTION else '‚ùå NO'}\")\n",
    "print(f\"   Authentication: {'‚úÖ Auto' if IS_CLOUD_EXECUTION else 'üîß Manual Required'}\")\n",
    "\n",
    "if IS_CLOUD_EXECUTION:\n",
    "    print(f\"   üí° All notebooks will run with full cloud integration!\")\n",
    "else:\n",
    "    print(f\"   üí° Some cloud features may require additional setup\")\n",
    "\n",
    "# Store environment info for other notebooks\n",
    "cloud_config = {\n",
    "    'environment': CLOUD_ENVIRONMENT,\n",
    "    'is_cloud': IS_CLOUD_EXECUTION,\n",
    "    'auto_auth': IS_CLOUD_EXECUTION,\n",
    "    'project_auto_detected': os.environ.get('GOOGLE_CLOUD_PROJECT') is not None\n",
    "}\n",
    "\n",
    "print(f\"\\n‚úÖ Cloud environment configuration stored for all notebooks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
