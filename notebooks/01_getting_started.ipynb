{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "968f17ae",
   "metadata": {},
   "source": [
    "# 01 - Getting Started: Environment Setup & Validation\n",
    "\n",
    "This notebook will guide you through setting up and validating your Google Cloud MLOps environment.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This is **Phase 1** of our MLOps pipeline project. We'll:\n",
    "\n",
    "1. **Validate Python Environment** - Check required packages\n",
    "2. **Configure Google Cloud Authentication** - Set up credentials\n",
    "3. **Verify API Access** - Ensure required services are enabled\n",
    "4. **Set Up Cloud Storage** - Create and configure GCS bucket\n",
    "5. **Test Vertex AI Connectivity** - Verify ML platform access\n",
    "6. **Prepare Sample Dataset** - Download and process Iris dataset\n",
    "7. **Environment Summary** - Validate complete setup\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7364ead",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Section 1: Python Environment Validation\n",
    "\n",
    "First, let's verify our Python environment and install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72419567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.13.7 (main, Aug 14 2025, 11:12:11) [Clang 17.0.0 (clang-1700.0.13.3)]\n",
      "Python executable: /Users/farishussain/GCP_MLOps/venv/bin/python\n",
      "Current working directory: /Users/farishussain/GCP_MLOps/notebooks\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Current working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8e98eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements if needed\n",
    "# Uncomment the line below if you need to install packages\n",
    "# !pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b94b0512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Package Validation:\n",
      "  âœ… google.cloud.aiplatform: 1.128.0\n",
      "  âœ… google.cloud.storage: 3.6.0\n",
      "  âœ… pandas: 2.3.3\n",
      "  âœ… numpy: 2.3.5\n",
      "  âœ… sklearn: 1.7.2\n",
      "  âœ… kfp: 2.14.6\n"
     ]
    }
   ],
   "source": [
    "# Verify key packages\n",
    "import importlib\n",
    "\n",
    "required_packages = [\n",
    "    'google.cloud.aiplatform',\n",
    "    'google.cloud.storage', \n",
    "    'pandas',\n",
    "    'numpy',\n",
    "    'sklearn',\n",
    "    'kfp'\n",
    "]\n",
    "\n",
    "print(\"ğŸ“¦ Package Validation:\")\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        mod = importlib.import_module(package)\n",
    "        version = getattr(mod, '__version__', 'Unknown')\n",
    "        print(f\"  âœ… {package}: {version}\")\n",
    "    except ImportError:\n",
    "        print(f\"  âŒ {package}: Not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32c17e6",
   "metadata": {},
   "source": [
    "## ğŸ” Section 2: Google Cloud Authentication\n",
    "\n",
    "Set up authentication for Google Cloud services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6c1ecbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Project Configuration:\n",
      "  Project ID: mlops-295610\n",
      "  Region: us-central1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.cloud import aiplatform\n",
    "from google.auth import default\n",
    "\n",
    "# Initialize Vertex AI with your project settings\n",
    "PROJECT_ID = \"mlops-295610\"  # Replace with your project ID\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "# Set environment variable for project\n",
    "os.environ['GOOGLE_CLOUD_PROJECT'] = PROJECT_ID\n",
    "\n",
    "print(f\"ğŸ”§ Project Configuration:\")\n",
    "print(f\"  Project ID: {PROJECT_ID}\")\n",
    "print(f\"  Region: {REGION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8edcdc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Authentication successful!\n",
      "  Authenticated project: mlops-295610\n",
      "âœ… Vertex AI initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Test authentication\n",
    "try:\n",
    "    credentials, project = default()\n",
    "    print(f\"âœ… Authentication successful!\")\n",
    "    print(f\"  Authenticated project: {project}\")\n",
    "    \n",
    "    # Initialize Vertex AI\n",
    "    aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "    print(f\"âœ… Vertex AI initialized successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Authentication failed: {e}\")\n",
    "    print(\"\\nğŸ”§ Quick fix: Run the following command in your terminal:\")\n",
    "    print(\"  gcloud auth application-default login\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8040c3",
   "metadata": {},
   "source": [
    "## ğŸŒ Section 3: Google Cloud APIs Verification\n",
    "\n",
    "Verify that required Google Cloud APIs are enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab057b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Cloud Storage API: Working (0 buckets found)\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "from google.api_core import exceptions\n",
    "\n",
    "# Test Cloud Storage API\n",
    "try:\n",
    "    storage_client = storage.Client(project=PROJECT_ID)\n",
    "    # List first few buckets to test API access\n",
    "    buckets = list(storage_client.list_buckets())\n",
    "    print(f\"âœ… Cloud Storage API: Working ({len(buckets)} buckets found)\")\n",
    "except exceptions.Forbidden:\n",
    "    print(\"âŒ Cloud Storage API: Access denied - check IAM permissions\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Cloud Storage API: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b65430d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vertex AI API: Working (0 models found)\n"
     ]
    }
   ],
   "source": [
    "# Test Vertex AI API\n",
    "try:\n",
    "    # Try to list models to test Vertex AI API access\n",
    "    models = aiplatform.Model.list()\n",
    "    print(f\"âœ… Vertex AI API: Working ({len(models)} models found)\")\n",
    "except exceptions.Forbidden:\n",
    "    print(\"âŒ Vertex AI API: Access denied - check IAM permissions\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ… Vertex AI API: Working (empty project is normal)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fe4dde",
   "metadata": {},
   "source": [
    "## ğŸª£ Section 4: Cloud Storage Setup\n",
    "\n",
    "Create and configure the GCS bucket for our MLOps pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cc3fc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸª£ Setting up bucket: mlops-vertex-ai-bucket-295610\n",
      "âœ… Created bucket: gs://mlops-vertex-ai-bucket-295610\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "from google.api_core import exceptions\n",
    "\n",
    "BUCKET_NAME = f\"mlops-vertex-ai-bucket-{PROJECT_ID.split('-')[-1]}\"\n",
    "\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "print(f\"ğŸª£ Setting up bucket: {BUCKET_NAME}\")\n",
    "\n",
    "# Check if bucket exists\n",
    "try:\n",
    "    bucket = storage_client.bucket(BUCKET_NAME)\n",
    "    bucket.reload()  # This will raise exception if bucket doesn't exist\n",
    "    print(f\"âœ… Bucket already exists: gs://{BUCKET_NAME}\")\n",
    "except exceptions.NotFound:\n",
    "    # Create bucket\n",
    "    try:\n",
    "        bucket = storage_client.create_bucket(\n",
    "            BUCKET_NAME, \n",
    "            location=REGION\n",
    "        )\n",
    "        print(f\"âœ… Created bucket: gs://{BUCKET_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to create bucket: {e}\")\n",
    "        print(\"Note: Bucket names must be globally unique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7b6280b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“ Created folder: data/\n",
      "  ğŸ“ Created folder: models/\n",
      "  ğŸ“ Created folder: pipelines/\n",
      "  ğŸ“ Created folder: outputs/\n",
      "  ğŸ“ Created folder: artifacts/\n",
      "\n",
      "âœ… Bucket structure created successfully!\n",
      "   Bucket URL: https://console.cloud.google.com/storage/browser/mlops-vertex-ai-bucket-295610\n"
     ]
    }
   ],
   "source": [
    "# Create folder structure in bucket\n",
    "folders = ['data/', 'models/', 'pipelines/', 'outputs/', 'artifacts/']\n",
    "\n",
    "for folder in folders:\n",
    "    blob = bucket.blob(folder + '.gitkeep')\n",
    "    try:\n",
    "        blob.upload_from_string('')\n",
    "        print(f\"  ğŸ“ Created folder: {folder}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Failed to create {folder}: {e}\")\n",
    "\n",
    "print(f\"\\nâœ… Bucket structure created successfully!\")\n",
    "print(f\"   Bucket URL: https://console.cloud.google.com/storage/browser/{BUCKET_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e68f37",
   "metadata": {},
   "source": [
    "## ğŸ¤– Section 5: Vertex AI Connectivity Test\n",
    "\n",
    "Test our connection to Vertex AI and explore available services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd6c6f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Vertex AI Status:\n",
      "  Project: mlops-295610\n",
      "  Location: us-central1\n",
      "\n",
      "ğŸ“Š Current Resources:\n",
      "  Datasets: 0\n",
      "  Models: 0\n",
      "  Endpoints: 0\n",
      "\n",
      "âœ… Vertex AI connectivity confirmed!\n"
     ]
    }
   ],
   "source": [
    "# Test Vertex AI connection and list available resources\n",
    "print(f\"ğŸ¤– Vertex AI Status:\")\n",
    "print(f\"  Project: {aiplatform.initializer.global_config.project}\")\n",
    "print(f\"  Location: {aiplatform.initializer.global_config.location}\")\n",
    "\n",
    "# List existing resources (will be empty for new projects)\n",
    "try:\n",
    "    datasets = aiplatform.TabularDataset.list()\n",
    "    models = aiplatform.Model.list()\n",
    "    endpoints = aiplatform.Endpoint.list()\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Current Resources:\")\n",
    "    print(f\"  Datasets: {len(datasets)}\")\n",
    "    print(f\"  Models: {len(models)}\")\n",
    "    print(f\"  Endpoints: {len(endpoints)}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Vertex AI connectivity confirmed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Vertex AI connection issue: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e80287f",
   "metadata": {},
   "source": [
    "## ğŸ“Š Section 6: Sample Dataset Preparation\n",
    "\n",
    "Download and prepare the Iris dataset for our MLOps pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e960ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Loading Iris dataset...\n",
      "\n",
      "ğŸ“ˆ Dataset Summary:\n",
      "  Shape: (150, 6)\n",
      "  Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "  Classes: [np.str_('setosa'), np.str_('versicolor'), np.str_('virginica')]\n",
      "  \n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "2                4.7               3.2                1.3               0.2   \n",
      "3                4.6               3.1                1.5               0.2   \n",
      "4                5.0               3.6                1.4               0.2   \n",
      "\n",
      "   target target_name  \n",
      "0       0      setosa  \n",
      "1       0      setosa  \n",
      "2       0      setosa  \n",
      "3       0      setosa  \n",
      "4       0      setosa  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "import pickle\n",
    "\n",
    "# Load Iris dataset\n",
    "print(\"ğŸ“Š Loading Iris dataset...\")\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "df['target'] = y\n",
    "df['target_name'] = df['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Dataset Summary:\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Features: {list(iris.feature_names)}\")\n",
    "print(f\"  Classes: {list(iris.target_names)}\")\n",
    "print(f\"  \\n{df.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac979518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved CSV: ../data/iris_dataset.csv\n",
      "âœ… Saved NumPy: ../data/iris_dataset.npz\n",
      "âœ… Saved metadata: ../data/iris_metadata.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save dataset locally\n",
    "data_dir = Path('../data')\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save as CSV\n",
    "csv_path = data_dir / 'iris_dataset.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"âœ… Saved CSV: {csv_path}\")\n",
    "\n",
    "# Save as NumPy arrays\n",
    "npz_path = data_dir / 'iris_dataset.npz'\n",
    "np.savez(npz_path, X=X, y=y, feature_names=iris.feature_names, target_names=iris.target_names)\n",
    "print(f\"âœ… Saved NumPy: {npz_path}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'name': 'iris',\n",
    "    'description': 'Iris flower classification dataset',\n",
    "    'n_samples': len(X),\n",
    "    'n_features': X.shape[1],\n",
    "    'n_classes': len(iris.target_names),\n",
    "    'feature_names': iris.feature_names,\n",
    "    'target_names': iris.target_names.tolist()\n",
    "}\n",
    "\n",
    "metadata_path = data_dir / 'iris_metadata.pkl'\n",
    "with open(metadata_path, 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "print(f\"âœ… Saved metadata: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7a0bd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â˜ï¸ Uploading dataset to GCS...\n",
      "âœ… Uploaded: gs://mlops-vertex-ai-bucket-295610/data/iris_dataset.csv\n",
      "âœ… Uploaded: gs://mlops-vertex-ai-bucket-295610/data/iris_dataset.npz\n",
      "âœ… Uploaded: gs://mlops-vertex-ai-bucket-295610/data/iris_metadata.pkl\n",
      "\n",
      "ğŸ‰ Dataset preparation complete!\n"
     ]
    }
   ],
   "source": [
    "# Upload dataset to Cloud Storage\n",
    "print(f\"â˜ï¸ Uploading dataset to GCS...\")\n",
    "\n",
    "# Upload CSV file\n",
    "blob = bucket.blob('data/iris_dataset.csv')\n",
    "blob.upload_from_filename(str(csv_path))\n",
    "print(f\"âœ… Uploaded: gs://{BUCKET_NAME}/data/iris_dataset.csv\")\n",
    "\n",
    "# Upload NumPy file\n",
    "blob = bucket.blob('data/iris_dataset.npz')\n",
    "blob.upload_from_filename(str(npz_path))\n",
    "print(f\"âœ… Uploaded: gs://{BUCKET_NAME}/data/iris_dataset.npz\")\n",
    "\n",
    "# Upload metadata\n",
    "blob = bucket.blob('data/iris_metadata.pkl')\n",
    "blob.upload_from_filename(str(metadata_path))\n",
    "print(f\"âœ… Uploaded: gs://{BUCKET_NAME}/data/iris_metadata.pkl\")\n",
    "\n",
    "print(f\"\\nğŸ‰ Dataset preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504d5393",
   "metadata": {},
   "source": [
    "## âœ… Section 7: Environment Summary\n",
    "\n",
    "Final validation of our complete environment setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25864487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Final Environment Validation\n",
      "==================================================\n",
      "\n",
      "ğŸ“‹ Configuration:\n",
      "  Project ID: mlops-295610\n",
      "  Region: us-central1\n",
      "  Bucket: mlops-vertex-ai-bucket-295610\n",
      "  Model Name: iris-classifier\n",
      "\n",
      "ğŸ”§ GCP Environment:\n",
      "  âœ… Gcloud Cli: True\n",
      "  âŒ Application Credentials: False\n",
      "  âœ… Project Id: True\n",
      "\n",
      "ğŸ“Š Local Datasets:\n",
      "  âœ… iris_dataset.csv: True\n",
      "  âœ… iris_dataset.npz: True\n",
      "  âœ… iris_metadata.pkl: True\n",
      "\n",
      "â˜ï¸ Cloud Storage:\n",
      "  âœ… gs://mlops-vertex-ai-bucket-295610/data/iris_dataset.csv (4189 bytes)\n",
      "  âœ… gs://mlops-vertex-ai-bucket-295610/data/iris_dataset.npz (7396 bytes)\n",
      "  âœ… gs://mlops-vertex-ai-bucket-295610/data/iris_metadata.pkl (275 bytes)\n",
      "\n",
      "ğŸ‰ Phase 1 Complete! Environment is ready for Phase 2.\n",
      "\n",
      "Next step: Create data processing pipeline notebook\n"
     ]
    }
   ],
   "source": [
    "# Import our project utilities\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from config import config\n",
    "from utils import validate_gcp_environment\n",
    "\n",
    "print(\"ğŸ” Final Environment Validation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Validate configuration\n",
    "print(f\"\\nğŸ“‹ Configuration:\")\n",
    "print(f\"  Project ID: {config.project_id}\")\n",
    "print(f\"  Region: {config.region}\")\n",
    "print(f\"  Bucket: {config.bucket_name}\")\n",
    "print(f\"  Model Name: {config.model_name}\")\n",
    "\n",
    "# Validate GCP environment\n",
    "validation = validate_gcp_environment()\n",
    "print(f\"\\nğŸ”§ GCP Environment:\")\n",
    "for component, status in validation.items():\n",
    "    status_icon = \"âœ…\" if status else \"âŒ\"\n",
    "    print(f\"  {status_icon} {component.replace('_', ' ').title()}: {status}\")\n",
    "\n",
    "# Check datasets\n",
    "data_files = [\n",
    "    '../data/iris_dataset.csv',\n",
    "    '../data/iris_dataset.npz', \n",
    "    '../data/iris_metadata.pkl'\n",
    "]\n",
    "\n",
    "print(f\"\\nğŸ“Š Local Datasets:\")\n",
    "for file_path in data_files:\n",
    "    exists = Path(file_path).exists()\n",
    "    status_icon = \"âœ…\" if exists else \"âŒ\"\n",
    "    print(f\"  {status_icon} {Path(file_path).name}: {exists}\")\n",
    "\n",
    "# Check GCS bucket contents\n",
    "print(f\"\\nâ˜ï¸ Cloud Storage:\")\n",
    "try:\n",
    "    blobs = list(bucket.list_blobs(prefix='data/'))\n",
    "    for blob in blobs:\n",
    "        if not blob.name.endswith('.gitkeep'):\n",
    "            print(f\"  âœ… gs://{BUCKET_NAME}/{blob.name} ({blob.size} bytes)\")\n",
    "except Exception as e:\n",
    "    print(f\"  âŒ Error accessing bucket: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ Phase 1 Complete! Environment is ready for Phase 2.\")\n",
    "print(f\"\\nNext step: Create data processing pipeline notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc97b6fe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Summary\n",
    "\n",
    "You have successfully completed **Phase 1: Environment Setup & Foundation**!\n",
    "\n",
    "### What we accomplished:\n",
    "\n",
    "âœ… **Python Environment** - Validated all required packages  \n",
    "âœ… **Google Cloud Authentication** - Set up project credentials  \n",
    "âœ… **API Access** - Verified Vertex AI and Cloud Storage APIs  \n",
    "âœ… **Cloud Storage** - Created bucket with proper structure  \n",
    "âœ… **Dataset Preparation** - Downloaded and uploaded Iris dataset  \n",
    "âœ… **Project Configuration** - Set up config files and utilities  \n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "Now you're ready to move to **Phase 2: Data Pipeline Implementation**\n",
    "\n",
    "- Create notebook: `02_data_processing_pipeline.ipynb`\n",
    "- Implement data preprocessing and validation\n",
    "- Set up train/test splits\n",
    "- Create data processing components\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ”— Useful Links:**\n",
    "- [Google Cloud Console](https://console.cloud.google.com)\n",
    "- [Vertex AI Console](https://console.cloud.google.com/vertex-ai)\n",
    "- [Cloud Storage Browser](https://console.cloud.google.com/storage/browser)\n",
    "- [Project Documentation](../README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
