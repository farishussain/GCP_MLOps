{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6cf84b7",
   "metadata": {},
   "source": [
    "# Model Training Pipeline\n",
    "\n",
    "This notebook demonstrates the complete model training pipeline including:\n",
    "- Loading preprocessed data\n",
    "- Training multiple ML algorithms\n",
    "- Hyperparameter tuning and optimization\n",
    "- Model evaluation and comparison\n",
    "- Performance visualization and analysis\n",
    "- Model selection and persistence\n",
    "\n",
    "**Author:** MLOps Team  \n",
    "**Version:** 1.0.0  \n",
    "**Date:** November 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f76ea57",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Import necessary libraries and configure the training environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f8a9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Standard ML libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Configure warnings and display\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")\n",
    "print(f\"ðŸ“… Notebook started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc39b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic configuration setup\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Define paths\n",
    "DATA_DIR = Path(\"../data\")\n",
    "PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
    "MODELS_DIR = Path(\"../models\")\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"ðŸ”§ Configuration setup complete\")\n",
    "print(f\"ðŸŽ² Random state: {RANDOM_STATE}\")\n",
    "print(f\"ðŸ“‚ Data directory: {PROCESSED_DATA_DIR}\")\n",
    "print(f\"ðŸ’¾ Models directory: {MODELS_DIR}\")\n",
    "\n",
    "# Verify processed data exists\n",
    "if not PROCESSED_DATA_DIR.exists():\n",
    "    print(\"âŒ Processed data directory not found!\")\n",
    "    print(\"Please run the data processing pipeline first (02_data_processing_pipeline.ipynb)\")\n",
    "else:\n",
    "    print(\"âœ… Processed data directory found\")\n",
    "\n",
    "print(\"âœ… Model training environment configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc80aa88",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data\n",
    "\n",
    "Load the training and test datasets that were prepared in the data processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d136ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed datasets\n",
    "train_file = PROCESSED_DATA_DIR / \"iris_train.csv\"\n",
    "test_file = PROCESSED_DATA_DIR / \"iris_test.csv\"\n",
    "metadata_file = PROCESSED_DATA_DIR / \"preprocessing_metadata.pkl\"\n",
    "\n",
    "if not train_file.exists() or not test_file.exists():\n",
    "    print(\"âŒ Processed data files not found!\")\n",
    "    print(\"Please run the data processing pipeline first (02_data_processing_pipeline.ipynb)\")\n",
    "    raise FileNotFoundError(\"Processed data files missing\")\n",
    "\n",
    "# Load training and test data\n",
    "train_data = pd.read_csv(train_file)\n",
    "test_data = pd.read_csv(test_file)\n",
    "\n",
    "print(f\"ðŸ“Š Training data shape: {train_data.shape}\")\n",
    "print(f\"ðŸ“Š Test data shape: {test_data.shape}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nðŸ” Training data preview:\")\n",
    "print(train_data.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nðŸ” Missing values check:\")\n",
    "print(f\"Training data: {train_data.isnull().sum().sum()} missing values\")\n",
    "print(f\"Test data: {test_data.isnull().sum().sum()} missing values\")\n",
    "\n",
    "print(f\"\\nâœ… Loaded training data: {train_data.shape}, test data: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74d901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and targets\n",
    "target_column = 'target'  # Our processed data uses 'target' column\n",
    "\n",
    "# Training data\n",
    "X_train = train_data.drop(columns=[target_column])\n",
    "y_train = train_data[target_column]\n",
    "\n",
    "# Test data\n",
    "X_test = test_data.drop(columns=[target_column])\n",
    "y_test = test_data[target_column]\n",
    "\n",
    "print(f\"ðŸŽ¯ Features shape - Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "print(f\"ðŸŽ¯ Target shape - Train: {y_train.shape}, Test: {y_test.shape}\")\n",
    "\n",
    "# Display feature information\n",
    "print(\"\\nðŸ“‹ Feature columns:\")\n",
    "for i, col in enumerate(X_train.columns):\n",
    "    print(f\"{i+1}. {col}\")\n",
    "\n",
    "# Display target classes\n",
    "unique_classes = sorted(y_train.unique())\n",
    "print(f\"\\nðŸ·ï¸ Target classes: {unique_classes}\")\n",
    "print(f\"ðŸ”¢ Number of classes: {len(unique_classes)}\")\n",
    "\n",
    "# Class distribution\n",
    "print(\"\\nðŸ“Š Class distribution in training data:\")\n",
    "class_counts = y_train.value_counts().sort_index()\n",
    "for class_idx, count in class_counts.items():\n",
    "    percentage = (count / len(y_train)) * 100\n",
    "    # Map numeric classes to iris species\n",
    "    class_names = {0: 'setosa', 1: 'versicolor', 2: 'virginica'}\n",
    "    class_name = class_names.get(class_idx, f'class_{class_idx}')\n",
    "    print(f\"  {class_name} (class {class_idx}): {count} samples ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nâœ… Prepared features and targets - Classes: {unique_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0fc245",
   "metadata": {},
   "source": [
    "## 3. Data Exploration and Visualization\n",
    "\n",
    "Understand the data distribution and relationships before training models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4513eab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics for features\n",
    "print(\"ðŸ“ˆ Feature Statistics:\")\n",
    "print(X_train.describe())\n",
    "\n",
    "# Feature correlations\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = X_train.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5)\n",
    "plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ” Highly correlated feature pairs (|correlation| > 0.7):\")\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_val = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.7:\n",
    "            feature1 = correlation_matrix.columns[i]\n",
    "            feature2 = correlation_matrix.columns[j]\n",
    "            high_corr_pairs.append((feature1, feature2, corr_val))\n",
    "            print(f\"  {feature1} â†” {feature2}: {corr_val:.3f}\")\n",
    "\n",
    "if not high_corr_pairs:\n",
    "    print(\"  No highly correlated feature pairs found.\")\n",
    "\n",
    "print(\"âœ… Completed basic data exploration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f261da92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions by class\n",
    "n_features = len(X_train.columns)\n",
    "n_cols = 2\n",
    "n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "if n_rows > 1:\n",
    "    axes = axes.ravel()\n",
    "else:\n",
    "    axes = [axes] if n_cols == 1 else axes\n",
    "\n",
    "for i, feature in enumerate(X_train.columns):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Create combined data for plotting\n",
    "    combined_data = pd.DataFrame({\n",
    "        feature: X_train[feature],\n",
    "        'target': y_train\n",
    "    })\n",
    "    \n",
    "    # Box plot by class\n",
    "    sns.boxplot(data=combined_data, x='target', y=feature, ax=ax)\n",
    "    ax.set_title(f'Distribution of {feature} by Class', fontweight='bold')\n",
    "    ax.set_xlabel('Class')\n",
    "    \n",
    "    # Add class labels\n",
    "    ax.set_xticklabels(['setosa', 'versicolor', 'virginica'])\n",
    "\n",
    "# Remove empty subplots\n",
    "for i in range(n_features, len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Visualized feature distributions by class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac50b0e",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation\n",
    "\n",
    "Train multiple machine learning models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2012e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configurations\n",
    "model_configs = {\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "        'param_grid': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "        'param_grid': {\n",
    "            'C': [0.1, 1.0, 10.0, 100.0],\n",
    "            'solver': ['liblinear', 'lbfgs']\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': SVC(random_state=RANDOM_STATE, probability=True),\n",
    "        'param_grid': {\n",
    "            'C': [0.1, 1.0, 10.0],\n",
    "            'kernel': ['linear', 'rbf', 'poly'],\n",
    "            'gamma': ['scale', 'auto']\n",
    "        }\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "        'param_grid': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"ðŸ”§ Model Configurations:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"\\nðŸ“‹ {model_name}\")\n",
    "    print(f\"   Algorithm: {config['model'].__class__.__name__}\")\n",
    "    print(f\"   Hyperparameters to tune: {list(config['param_grid'].keys())}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Total models configured: {len(model_configs)}\")\n",
    "print(\"âœ… Model configurations ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc21c53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models with hyperparameter tuning\n",
    "print(\"ðŸš€ Starting model training with hyperparameter tuning...\")\n",
    "print(\"This may take several minutes depending on the parameter grids.\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = datetime.now()\n",
    "training_results = {}\n",
    "\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"\\nðŸ”„ Training {model_name}...\")\n",
    "    model_start_time = datetime.now()\n",
    "    \n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=config['model'],\n",
    "        param_grid=config['param_grid'],\n",
    "        cv=5,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Make predictions\n",
    "    train_predictions = best_model.predict(X_train)\n",
    "    test_predictions = best_model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "    test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "    \n",
    "    # Cross-validation scores\n",
    "    cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # Additional metrics\n",
    "    precision = precision_score(y_test, test_predictions, average='weighted')\n",
    "    recall = recall_score(y_test, test_predictions, average='weighted')\n",
    "    f1 = f1_score(y_test, test_predictions, average='weighted')\n",
    "    \n",
    "    # Training time\n",
    "    model_end_time = datetime.now()\n",
    "    training_time = (model_end_time - model_start_time).total_seconds()\n",
    "    \n",
    "    # Store results\n",
    "    training_results[model_name] = {\n",
    "        'model': best_model,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'training_time': training_time,\n",
    "        'test_predictions': test_predictions\n",
    "    }\n",
    "    \n",
    "    print(f\"   âœ… {model_name} completed in {training_time:.2f}s - Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "end_time = datetime.now()\n",
    "total_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(\"\\nâœ… Model training completed!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"â±ï¸  Total training time: {total_time:.2f} seconds\")\n",
    "print(f\"ðŸ† Models trained successfully: {len(training_results)}\")\n",
    "\n",
    "# Display quick results summary\n",
    "print(\"\\nðŸ“Š Quick Results Summary:\")\n",
    "for model_name, result in training_results.items():\n",
    "    print(f\"  {model_name}: {result['test_accuracy']:.4f} accuracy\")\n",
    "\n",
    "print(f\"\\nâœ… Successfully trained {len(training_results)} models in {total_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f120e9a",
   "metadata": {},
   "source": [
    "# Create comprehensive model comparison\n",
    "comparison_data = []\n",
    "for model_name, result in training_results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Train_Accuracy': result['train_accuracy'],\n",
    "        'Test_Accuracy': result['test_accuracy'],\n",
    "        'CV_Mean': result['cv_mean'],\n",
    "        'CV_Std': result['cv_std'],\n",
    "        'Precision': result['precision'],\n",
    "        'Recall': result['recall'],\n",
    "        'F1_Score': result['f1_score'],\n",
    "        'Training_Time': result['training_time']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"ðŸ“Š Model Performance Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Display with nice formatting\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Identify best performing models\n",
    "best_test_acc = comparison_df.loc[comparison_df['Test_Accuracy'].idxmax()]\n",
    "best_cv = comparison_df.loc[comparison_df['CV_Mean'].idxmax()]\n",
    "best_f1 = comparison_df.loc[comparison_df['F1_Score'].idxmax()]\n",
    "\n",
    "print(\"\\nðŸ† Top Performers:\")\n",
    "print(f\"  ðŸŽ¯ Best Test Accuracy: {best_test_acc['Model']} ({best_test_acc['Test_Accuracy']:.4f})\")\n",
    "print(f\"  ðŸ“ˆ Best Cross-Validation: {best_cv['Model']} ({best_cv['CV_Mean']:.4f})\")\n",
    "print(f\"  âš–ï¸ Best F1-Score: {best_f1['Model']} ({best_f1['F1_Score']:.4f})\")\n",
    "\n",
    "# Get the best overall model (highest test accuracy)\n",
    "best_model_name = best_test_acc['Model']\n",
    "best_result = training_results[best_model_name]\n",
    "\n",
    "print(f\"\\nðŸ† Selected Best Model: {best_model_name}\")\n",
    "print(f\"âœ… Best model by test accuracy: {best_model_name} ({best_test_acc['Test_Accuracy']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82f2c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive model comparison\n",
    "comparison_data = []\n",
    "for model_name, result in training_results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Train_Accuracy': result['train_accuracy'],\n",
    "        'Test_Accuracy': result['test_accuracy'],\n",
    "        'CV_Mean': result['cv_mean'],\n",
    "        'CV_Std': result['cv_std'],\n",
    "        'Precision': result['precision'],\n",
    "        'Recall': result['recall'],\n",
    "        'F1_Score': result['f1_score'],\n",
    "        'Training_Time': result['training_time']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"ðŸ“Š Model Performance Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Identify best performing models\n",
    "best_test_acc = comparison_df.loc[comparison_df['Test_Accuracy'].idxmax()]\n",
    "best_cv = comparison_df.loc[comparison_df['CV_Mean'].idxmax()]\n",
    "best_f1 = comparison_df.loc[comparison_df['F1_Score'].idxmax()]\n",
    "\n",
    "print(\"\\nðŸ† Top Performers:\")\n",
    "print(f\"  ðŸŽ¯ Best Test Accuracy: {best_test_acc['Model']} ({best_test_acc['Test_Accuracy']:.4f})\")\n",
    "print(f\"  ðŸ“ˆ Best Cross-Validation: {best_cv['Model']} ({best_cv['CV_Mean']:.4f})\")\n",
    "print(f\"  âš–ï¸ Best F1-Score: {best_f1['Model']} ({best_f1['F1_Score']:.4f})\")\n",
    "\n",
    "# Get the best overall model (highest test accuracy)\n",
    "best_model_name = best_test_acc['Model']\n",
    "best_result = training_results[best_model_name]\n",
    "best_model = training_results[best_model_name]['model']\n",
    "\n",
    "print(f\"\\nðŸ† Selected Best Model: {best_model_name}\")\n",
    "\n",
    "# Detailed results for best model\n",
    "print(f\"\\nðŸ“‹ Best Model Details:\")\n",
    "print(f\"   ðŸŽ¯ Test Accuracy: {best_result['test_accuracy']:.4f}\")\n",
    "print(f\"   ðŸ“ˆ Cross-Validation: {best_result['cv_mean']:.4f} Â± {best_result['cv_std']:.4f}\")\n",
    "print(f\"   ðŸ”§ Best Parameters: {best_result['best_params']}\")\n",
    "print(f\"   â±ï¸  Training Time: {best_result['training_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcbf394",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation and Visualization\n",
    "\n",
    "Detailed analysis of model performance with visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78daaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model comparison visualization\n",
    "print(\"ðŸ“ˆ Creating model performance comparison plot...\")\n",
    "\n",
    "# Prepare data for visualization\n",
    "metrics_map = {\n",
    "    'Test Accuracy': 'test_accuracy',\n",
    "    'Precision': 'precision', \n",
    "    'Recall': 'recall',\n",
    "    'F1 Score': 'f1_score'\n",
    "}\n",
    "\n",
    "models = list(training_results.keys())\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, (metric_name, metric_key) in enumerate(metrics_map.items()):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Get metric values\n",
    "    values = [training_results[model][metric_key] for model in models]\n",
    "    colors = ['gold' if model == best_model_name else 'skyblue' for model in models]\n",
    "    \n",
    "    # Create bar plot\n",
    "    bars = ax.bar(models, values, color=colors, alpha=0.8, edgecolor='black')\n",
    "    ax.set_title(f'Model Comparison - {metric_name}', fontweight='bold')\n",
    "    ax.set_ylabel(metric_name)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Set y-axis limits for better visualization\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Generated model comparison visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864cf7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check the actual structure of training_results\n",
    "print(\"Debug - Training Results Structure:\")\n",
    "for model_name, result in training_results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for key, value in result.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nBest model: {best_model_name}\")\n",
    "print(f\"Best result keys: {list(best_result.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3083536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance plots for models that support it\n",
    "print(\"ðŸ” Creating feature importance plots...\")\n",
    "\n",
    "# Only include models that have feature importance\n",
    "models_with_importance = {}\n",
    "for name, result in training_results.items():\n",
    "    if 'feature_importance' in result and result['feature_importance'] is not None:\n",
    "        models_with_importance[name] = result\n",
    "\n",
    "if models_with_importance:\n",
    "    n_models = len(models_with_importance)\n",
    "    n_cols = 2\n",
    "    n_rows = (n_models + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = [axes] if n_models == 1 else axes\n",
    "    else:\n",
    "        axes = axes.ravel()\n",
    "    \n",
    "    for idx, (model_name, result) in enumerate(models_with_importance.items()):\n",
    "        ax = axes[idx] if n_models > 1 else axes\n",
    "        \n",
    "        # Get feature importance from dictionary\n",
    "        feature_importance = result['feature_importance']\n",
    "        features = list(feature_importance.keys())\n",
    "        importance_values = list(feature_importance.values())\n",
    "        \n",
    "        # Create horizontal bar plot\n",
    "        y_pos = np.arange(len(features))\n",
    "        ax.barh(y_pos, importance_values, alpha=0.8, color='skyblue')\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(features)\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_xlabel('Importance Score')\n",
    "        ax.set_title(f'Feature Importance - {model_name}', fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Remove empty subplots\n",
    "    for idx in range(len(models_with_importance), len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"ðŸ“Š Feature importance plots created for {len(models_with_importance)} models\")\n",
    "else:\n",
    "    print(\"â„¹ï¸  No models with feature importance available\")\n",
    "\n",
    "print(\"âœ… Feature importance analysis completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36375b0",
   "metadata": {},
   "source": [
    "## 7. Model Selection and Analysis\n",
    "\n",
    "Select the best model and perform detailed analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3636fd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of the best model\n",
    "print(\"Best Model Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nSelected Model: {best_model_name}\")\n",
    "print(f\"Test Accuracy: {best_result['test_accuracy']:.4f}\")\n",
    "print(f\"Training Accuracy: {best_result['train_accuracy']:.4f}\")\n",
    "print(f\"Cross-Validation Score: {best_result['cv_mean']:.4f} Â± {best_result['cv_std']:.4f}\")\n",
    "print(f\"Training Time: {best_result['training_time']:.2f} seconds\")\n",
    "\n",
    "# Performance metrics\n",
    "print(f\"\\nDetailed Performance Metrics:\")\n",
    "print(f\"  â€¢ Precision: {best_result['precision']:.4f}\")\n",
    "print(f\"  â€¢ Recall: {best_result['recall']:.4f}\")\n",
    "print(f\"  â€¢ F1-Score: {best_result['f1_score']:.4f}\")\n",
    "\n",
    "# Best hyperparameters\n",
    "print(f\"\\nOptimized Hyperparameters:\")\n",
    "for param, value in best_result['best_params'].items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Performance insights\n",
    "print(f\"\\nPerformance Insights:\")\n",
    "accuracy_gap = abs(best_result['train_accuracy'] - best_result['test_accuracy'])\n",
    "print(f\"  â€¢ Training vs Test Accuracy Gap: {accuracy_gap:.4f}\")\n",
    "if accuracy_gap > 0.05:\n",
    "    print(f\"    WARNING: Large gap may indicate overfitting\")\n",
    "else:\n",
    "    print(f\"    GOOD: Good generalization capability\")\n",
    "\n",
    "print(f\"  â€¢ CV Standard Deviation: {best_result['cv_std']:.4f}\")\n",
    "if best_result['cv_std'] > 0.05:\n",
    "    print(f\"    WARNING: High variance across CV folds\")\n",
    "else:\n",
    "    print(f\"    GOOD: Stable performance across folds\")\n",
    "\n",
    "print(f\"\\nModel Ranking by Test Accuracy:\")\n",
    "# Sort all models by test accuracy\n",
    "sorted_models = sorted(training_results.items(), \n",
    "                      key=lambda x: x[1]['test_accuracy'], \n",
    "                      reverse=True)\n",
    "for i, (name, result) in enumerate(sorted_models, 1):\n",
    "    print(f\"  {i}. {name}: {result['test_accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nBest model analysis completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cff19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some prediction examples\n",
    "print(\"Prediction Examples (Best Model):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Make predictions with the best model\n",
    "best_model_predictions = best_model.predict(X_test)\n",
    "best_model_probabilities = None\n",
    "\n",
    "if hasattr(best_model, 'predict_proba'):\n",
    "    best_model_probabilities = best_model.predict_proba(X_test)\n",
    "\n",
    "# Show first 10 predictions\n",
    "print(\"\\nFirst 10 Test Predictions:\")\n",
    "print(f\"{'Index':<6} {'Actual':<12} {'Predicted':<12} {'Correct':<8}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for i in range(min(10, len(X_test))):\n",
    "    actual = y_test.iloc[i]\n",
    "    predicted = best_model_predictions[i]\n",
    "    is_correct = \"YES\" if actual == predicted else \"NO\"\n",
    "    \n",
    "    print(f\"{i:<6} {actual:<12} {predicted:<12} {is_correct:<8}\")\n",
    "\n",
    "# Accuracy breakdown by class\n",
    "print(\"\\nAccuracy by Class:\")\n",
    "from sklearn.metrics import classification_report\n",
    "class_report = classification_report(y_test, best_model_predictions, output_dict=True)\n",
    "\n",
    "for class_name in sorted(y_test.unique()):\n",
    "    if str(class_name) in class_report:\n",
    "        precision = class_report[str(class_name)]['precision']\n",
    "        recall = class_report[str(class_name)]['recall']\n",
    "        f1 = class_report[str(class_name)]['f1-score']\n",
    "        support = class_report[str(class_name)]['support']\n",
    "        \n",
    "        print(f\"  Class {class_name}:\")\n",
    "        print(f\"    Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f} ({int(support)} samples)\")\n",
    "\n",
    "print(\"\\nPrediction examples completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe430d1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5ab73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model and create metadata\n",
    "print(\"Saving best model and creating metadata...\")\n",
    "\n",
    "# Create timestamp for file naming\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Define file paths\n",
    "best_model_path = MODELS_DIR / f\"best_model_{timestamp}.pkl\"\n",
    "all_models_path = MODELS_DIR / f\"all_models_{timestamp}.pkl\"\n",
    "results_summary_path = MODELS_DIR / f\"training_results_{timestamp}.pkl\"\n",
    "\n",
    "# Save the best model\n",
    "with open(best_model_path, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "# Save all models\n",
    "all_models_dict = {name: result['model'] for name, result in training_results.items()}\n",
    "with open(all_models_path, 'wb') as f:\n",
    "    pickle.dump(all_models_dict, f)\n",
    "\n",
    "# Save training results\n",
    "with open(results_summary_path, 'wb') as f:\n",
    "    pickle.dump(training_results, f)\n",
    "\n",
    "# Create model metadata file\n",
    "model_metadata = {\n",
    "    'model_info': {\n",
    "        'name': best_model_name,\n",
    "        'algorithm': best_model_name,  # Using model name as algorithm\n",
    "        'version': '1.0.0',\n",
    "        'training_date': datetime.now().isoformat(),\n",
    "        'random_state': int(RANDOM_STATE)\n",
    "    },\n",
    "    'performance': {\n",
    "        'test_accuracy': float(best_result['test_accuracy']),\n",
    "        'cv_mean': float(best_result['cv_mean']),\n",
    "        'cv_std': float(best_result['cv_std']),\n",
    "        'precision': float(best_result['precision']),\n",
    "        'recall': float(best_result['recall']),\n",
    "        'f1_score': float(best_result['f1_score']),\n",
    "        'training_time': float(best_result['training_time'])\n",
    "    },\n",
    "    'data_info': {\n",
    "        'training_samples': int(len(X_train)),\n",
    "        'test_samples': int(len(X_test)),\n",
    "        'features': list(X_train.columns),\n",
    "        'target_classes': [int(x) for x in sorted(y_train.unique())],\n",
    "        'feature_count': int(len(X_train.columns)),\n",
    "        'class_count': int(len(y_train.unique()))\n",
    "    },\n",
    "    'hyperparameters': best_result['best_params'],\n",
    "    'files': {\n",
    "        'model_file': str(best_model_path),\n",
    "        'results_file': str(results_summary_path),\n",
    "        'all_models_file': str(all_models_path)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = MODELS_DIR / f\"model_metadata_{timestamp}.json\"\n",
    "import json\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "\n",
    "print(f\"Model metadata saved: {metadata_path}\")\n",
    "\n",
    "# Display metadata summary\n",
    "print(\"\\nModel Deployment Information:\")\n",
    "print(f\"  Model Name: {model_metadata['model_info']['name']}\")\n",
    "print(f\"  Algorithm: {model_metadata['model_info']['algorithm']}\")\n",
    "print(f\"  Test Accuracy: {model_metadata['performance']['test_accuracy']:.4f}\")\n",
    "print(f\"  Features: {model_metadata['data_info']['feature_count']}\")\n",
    "print(f\"  Classes: {model_metadata['data_info']['class_count']}\")\n",
    "print(f\"  Model File: {best_model_path.name}\")\n",
    "\n",
    "print(\"\\nModel saving completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d96797",
   "metadata": {},
   "source": [
    "# Create comprehensive evaluation report\n",
    "evaluation_dir = Path(\"../evaluation_reports\")\n",
    "\n",
    "print(\"ðŸ“„ Creating comprehensive evaluation report...\")\n",
    "print(\"This will include all visualizations and detailed analysis.\")\n",
    "\n",
    "report_path = evaluator.create_evaluation_report(\n",
    "    training_results,\n",
    "    output_dir=str(evaluation_dir)\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Comprehensive evaluation report created!\")\n",
    "print(f\"ðŸ“ Report location: {report_path}\")\n",
    "print(f\"ðŸ“ Report directory: {evaluation_dir}\")\n",
    "\n",
    "# List generated files\n",
    "if evaluation_dir.exists():\n",
    "    report_files = list(evaluation_dir.glob(\"*\"))\n",
    "    print(f\"\\nðŸ“‹ Generated files ({len(report_files)}):\")\n",
    "    for file_path in sorted(report_files):\n",
    "        file_size = file_path.stat().st_size / 1024  # KB\n",
    "        print(f\"  ðŸ“„ {file_path.name} ({file_size:.1f} KB)\")\n",
    "\n",
    "logger.info(f\"Comprehensive evaluation report generated: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877f75e8",
   "metadata": {},
   "source": [
    "# Final training summary\n",
    "print(\"ðŸŽ¯ MODEL TRAINING PIPELINE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "end_time = datetime.now()\n",
    "notebook_duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(f\"\\nâ±ï¸  Pipeline Duration: {notebook_duration:.1f} seconds\")\n",
    "print(f\"ðŸ“Š Total Models Trained: {len(training_results)}\")\n",
    "print(f\"ðŸ† Best Model: {best_model_name}\")\n",
    "print(f\"ðŸŽ¯ Best Accuracy: {best_result.test_accuracy:.4f}\")\n",
    "print(f\"ðŸ“ˆ Cross-Validation Score: {best_result.cross_val_mean:.4f} Â± {best_result.cross_val_std:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ’¾ Saved Artifacts:\")\n",
    "print(f\"  ðŸ“¦ Best Model: {best_model_path.name}\")\n",
    "print(f\"  ðŸ“¦ All Models: {all_models_path.name}\")\n",
    "print(f\"  ðŸ“‹ Training Results: {results_summary_path.name}\")\n",
    "print(f\"  ðŸ“‹ Model Metadata: {metadata_path.name}\")\n",
    "print(f\"  ðŸ“„ Evaluation Report: {Path(report_path).name}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Model Performance Summary:\")\n",
    "for model_name, result in sorted(training_results.items(), \n",
    "                               key=lambda x: x[1].test_accuracy, reverse=True):\n",
    "    status = \"ðŸ¥‡\" if model_name == best_model_name else \"  \"\n",
    "    print(f\"  {status} {model_name}: {result.test_accuracy:.4f} accuracy\")\n",
    "\n",
    "print(f\"\\nðŸš€ Next Steps:\")\n",
    "print(f\"  1ï¸âƒ£ Deploy the best model using the model deployment pipeline\")\n",
    "print(f\"  2ï¸âƒ£ Set up model monitoring and performance tracking\")\n",
    "print(f\"  3ï¸âƒ£ Consider ensemble methods for improved performance\")\n",
    "print(f\"  4ï¸âƒ£ Implement automated retraining pipeline\")\n",
    "print(f\"  5ï¸âƒ£ Create inference endpoints for model serving\")\n",
    "\n",
    "print(f\"\\nâœ… Model training pipeline completed successfully!\")\n",
    "print(f\"ðŸ“ All artifacts saved to: {MODELS_DIR}\")\n",
    "\n",
    "logger.info(f\"Model training pipeline completed successfully - Best: {best_model_name} ({best_result.test_accuracy:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0617f387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up and final notes\n",
    "plt.close('all')  # Close all matplotlib figures\n",
    "\n",
    "print(\"Resources cleaned up\")\n",
    "print(\"Model training notebook completed successfully!\")\n",
    "print(\"\\nRelated Notebooks:\")\n",
    "print(\"  01_getting_started.ipynb - Environment setup\")\n",
    "print(\"  02_data_processing_pipeline.ipynb - Data preparation\")\n",
    "print(\"  03_model_training.ipynb - Current notebook\")\n",
    "print(\"  04_vertex_ai_training.ipynb - Cloud training (Next)\")\n",
    "print(\"  05_model_deployment.ipynb - Model deployment (Next)\")\n",
    "\n",
    "print(\"\\nFinal Summary:\")\n",
    "print(f\"  Best Model: {best_model_name}\")\n",
    "print(f\"  Test Accuracy: {best_result['test_accuracy']:.4f}\")\n",
    "print(f\"  Training Time: {best_result['training_time']:.2f} seconds\")\n",
    "print(f\"  Models Saved: {MODELS_DIR}\")\n",
    "\n",
    "print(\"\\nModel training notebook completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
