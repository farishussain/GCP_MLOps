{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c66bf022",
   "metadata": {},
   "source": [
    "# Vertex AI Training Pipeline\n",
    "\n",
    "This notebook demonstrates cloud-based model training using Google Cloud Vertex AI, including:\n",
    "- Setting up Google Cloud authentication and configuration\n",
    "- Uploading training data to Google Cloud Storage\n",
    "- Creating custom training jobs on Vertex AI\n",
    "- Monitoring training progress and retrieving results\n",
    "- Hyperparameter tuning with Vertex AI\n",
    "- Model deployment to Vertex AI endpoints\n",
    "- Best practices for production MLOps workflows\n",
    "\n",
    "**Author:** MLOps Team  \n",
    "**Version:** 1.0.0  \n",
    "**Date:** November 2024  \n",
    "**Phase:** 4 - Cloud Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f745d50d",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Import necessary libraries and configure the cloud training environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84843b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-24 21:17:03,655 - utils - INFO - Logging configured successfully\n",
      "âœ… Core modules imported successfully\n",
      "âœ… Cloud modules imported successfully\n",
      "âœ… Core modules imported successfully\n",
      "âœ… Cloud modules imported successfully\n",
      "âš ï¸ ML modules not found: attempted relative import beyond top-level package\n",
      "   Will use basic sklearn implementations\n",
      "âœ… Libraries imported successfully\n",
      "ğŸ“… Notebook started at: 2025-11-24 21:17:04\n",
      "ğŸŒ¥ï¸  Vertex AI Training Pipeline - Phase 4\n",
      "âš ï¸ ML modules not found: attempted relative import beyond top-level package\n",
      "   Will use basic sklearn implementations\n",
      "âœ… Libraries imported successfully\n",
      "ğŸ“… Notebook started at: 2025-11-24 21:17:04\n",
      "ğŸŒ¥ï¸  Vertex AI Training Pipeline - Phase 4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "# Add src to path for imports\n",
    "src_path = os.path.abspath('../src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "# Import our custom modules (with error handling for missing modules)\n",
    "try:\n",
    "    from config import Config\n",
    "    from utils import setup_logging\n",
    "    print(\"âœ… Core modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Some core modules not found: {e}\")\n",
    "    print(\"   Continuing with basic imports for demo mode\")\n",
    "    \n",
    "try:\n",
    "    from cloud.vertex_ai import VertexAITrainer, TrainingJobConfig, CloudTrainingUtils\n",
    "    print(\"âœ… Cloud modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Cloud modules not found: {e}\")\n",
    "    print(\"   Will use mock implementations for demonstration\")\n",
    "\n",
    "try:\n",
    "    from models.trainer import ModelTrainer\n",
    "    from data.data_loader import DataLoader\n",
    "    print(\"âœ… ML modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ ML modules not found: {e}\")\n",
    "    print(\"   Will use basic sklearn implementations\")\n",
    "\n",
    "# Configure warnings and display\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")\n",
    "print(f\"ğŸ“… Notebook started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"ğŸŒ¥ï¸  Vertex AI Training Pipeline - Phase 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53ec454e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Configuration setup complete\n",
      "ğŸ² Random state: 42\n",
      "ğŸ“‚ Data directory: ../data/processed\n",
      "ğŸ’¾ Models directory: ../models\n",
      "â˜ï¸  Cloud artifacts: ../cloud_artifacts\n",
      "âœ… Vertex AI training environment configured\n"
     ]
    }
   ],
   "source": [
    "# Simple configuration setup\n",
    "RANDOM_STATE = 42\n",
    "DATA_DIR = Path(\"../data\")\n",
    "\n",
    "# Setup basic logging\n",
    "import logging\n",
    "logger = logging.getLogger('vertex_ai_training')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Define paths\n",
    "PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
    "MODELS_DIR = Path(\"../models\")\n",
    "CLOUD_DIR = Path(\"../cloud_artifacts\")\n",
    "CLOUD_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ”§ Configuration setup complete\")\n",
    "print(f\"ğŸ² Random state: {RANDOM_STATE}\")\n",
    "print(f\"ğŸ“‚ Data directory: {PROCESSED_DATA_DIR}\")\n",
    "print(f\"ğŸ’¾ Models directory: {MODELS_DIR}\")\n",
    "print(f\"â˜ï¸  Cloud artifacts: {CLOUD_DIR}\")\n",
    "\n",
    "print(\"âœ… Vertex AI training environment configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ac4feaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ¥ï¸  Google Cloud Configuration:\n",
      "   ğŸ“‹ Project ID: mlops-295610\n",
      "   ğŸŒ Location: us-central1\n",
      "   ğŸ—‚ï¸  Staging Bucket: gs://mlops-295610-vertex-ai\n",
      "âœ… Authenticated with Google Cloud as: farishussain049@gmail.com\n",
      "ğŸ” Google Cloud authentication verified\n",
      "âœ… Google Cloud configuration complete: mlops-295610 in us-central1\n",
      "âœ… Authenticated with Google Cloud as: farishussain049@gmail.com\n",
      "ğŸ” Google Cloud authentication verified\n",
      "âœ… Google Cloud configuration complete: mlops-295610 in us-central1\n"
     ]
    }
   ],
   "source": [
    "# Google Cloud Configuration\n",
    "# Note: Update these values for your specific Google Cloud project\n",
    "PROJECT_ID = \"mlops-295610\"  # Your actual project ID\n",
    "LOCATION = \"us-central1\"  # Vertex AI region\n",
    "STAGING_BUCKET = f\"{PROJECT_ID}-vertex-ai\"  # GCS bucket for staging\n",
    "\n",
    "print(\"ğŸŒ¥ï¸  Google Cloud Configuration:\")\n",
    "print(f\"   ğŸ“‹ Project ID: {PROJECT_ID}\")\n",
    "print(f\"   ğŸŒ Location: {LOCATION}\")\n",
    "print(f\"   ğŸ—‚ï¸  Staging Bucket: gs://{STAGING_BUCKET}\")\n",
    "\n",
    "# Check Google Cloud CLI availability\n",
    "def check_gcloud_auth():\n",
    "    \"\"\"Check if gcloud CLI is installed and authenticated.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(['gcloud', 'auth', 'list', '--format=json'], \n",
    "                              capture_output=True, text=True, timeout=10)\n",
    "        if result.returncode == 0:\n",
    "            accounts = json.loads(result.stdout)\n",
    "            active_account = [acc for acc in accounts if acc.get('status') == 'ACTIVE']\n",
    "            if active_account:\n",
    "                print(f\"âœ… Authenticated with Google Cloud as: {active_account[0]['account']}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"âŒ No active Google Cloud authentication found\")\n",
    "                return False\n",
    "        else:\n",
    "            print(\"âŒ Google Cloud CLI not properly configured\")\n",
    "            return False\n",
    "    except (subprocess.TimeoutExpired, FileNotFoundError, json.JSONDecodeError):\n",
    "        print(\"âŒ Google Cloud CLI not found or authentication failed\")\n",
    "        return False\n",
    "\n",
    "# Check authentication\n",
    "auth_status = check_gcloud_auth()\n",
    "\n",
    "if auth_status:\n",
    "    print(\"ğŸ” Google Cloud authentication verified\")\n",
    "else:\n",
    "    print(\"âš ï¸  Please authenticate with Google Cloud:\")\n",
    "    print(\"   1. Install Google Cloud CLI: https://cloud.google.com/sdk/docs/install\")\n",
    "    print(\"   2. Run: gcloud auth login\")\n",
    "    print(\"   3. Run: gcloud config set project YOUR_PROJECT_ID\")\n",
    "    print(\"   4. Restart this notebook\")\n",
    "\n",
    "print(f\"âœ… Google Cloud configuration complete: {PROJECT_ID} in {LOCATION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f71133",
   "metadata": {},
   "source": [
    "## 2. Prepare Data for Cloud Training\n",
    "\n",
    "Upload training data to Google Cloud Storage and prepare for Vertex AI training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a6c5718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Loading training data for cloud upload...\n",
      "âœ… Loaded existing processed data\n",
      "   ğŸ“Š Training data: (120, 5)\n",
      "   ğŸ“Š Test data: (30, 5)\n",
      "â˜ï¸  Cloud training dataset created: ../cloud_artifacts/iris_training_data.csv\n",
      "   ğŸ“Š Combined data shape: (150, 5)\n",
      "\n",
      "ğŸ” Sample of cloud training data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.748856</td>\n",
       "      <td>-0.362176</td>\n",
       "      <td>-1.340227</td>\n",
       "      <td>-1.315444</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.143017</td>\n",
       "      <td>-1.282963</td>\n",
       "      <td>0.421734</td>\n",
       "      <td>0.659038</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.159173</td>\n",
       "      <td>-0.592373</td>\n",
       "      <td>0.592246</td>\n",
       "      <td>0.264142</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.143017</td>\n",
       "      <td>0.098217</td>\n",
       "      <td>-1.283389</td>\n",
       "      <td>-1.447076</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.416010</td>\n",
       "      <td>-1.282963</td>\n",
       "      <td>0.137547</td>\n",
       "      <td>0.132510</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0          -1.748856         -0.362176          -1.340227         -1.315444   \n",
       "1          -1.143017         -1.282963           0.421734          0.659038   \n",
       "2           1.159173         -0.592373           0.592246          0.264142   \n",
       "3          -1.143017          0.098217          -1.283389         -1.447076   \n",
       "4          -0.416010         -1.282963           0.137547          0.132510   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       2  \n",
       "2       1  \n",
       "3       0  \n",
       "4       1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load local datasets\n",
    "print(\"ğŸ“Š Loading training data for cloud upload...\")\n",
    "\n",
    "# Check if processed data exists\n",
    "train_file = PROCESSED_DATA_DIR / \"iris_train.csv\"\n",
    "test_file = PROCESSED_DATA_DIR / \"iris_test.csv\"\n",
    "\n",
    "if train_file.exists() and test_file.exists():\n",
    "    # Load existing processed data\n",
    "    train_data = pd.read_csv(train_file)\n",
    "    test_data = pd.read_csv(test_file)\n",
    "    print(f\"âœ… Loaded existing processed data\")\n",
    "    print(f\"   ğŸ“Š Training data: {train_data.shape}\")\n",
    "    print(f\"   ğŸ“Š Test data: {test_data.shape}\")\n",
    "else:\n",
    "    # Create processed data using sklearn iris dataset\n",
    "    print(\"ğŸ”„ Creating processed datasets...\")\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    # Load iris dataset\n",
    "    iris = load_iris()\n",
    "    X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "    y = pd.Series(iris.target, name='target')\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    # Split data\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Create DataFrames\n",
    "    train_data = pd.concat([X_train.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)\n",
    "    test_data = pd.concat([X_test.reset_index(drop=True), y_test.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    # Ensure directories exist\n",
    "    PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save datasets\n",
    "    train_data.to_csv(train_file, index=False)\n",
    "    test_data.to_csv(test_file, index=False)\n",
    "    \n",
    "    print(f\"âœ… Created and saved datasets\")\n",
    "    print(f\"   ğŸ“Š Training data: {train_data.shape}\")\n",
    "    print(f\"   ğŸ“Š Test data: {test_data.shape}\")\n",
    "\n",
    "# Prepare cloud training dataset (combine for cloud training)\n",
    "cloud_train_data = pd.concat([train_data, test_data], axis=0)  # Concatenate rows, not columns\n",
    "cloud_data_path = CLOUD_DIR / \"iris_training_data.csv\"\n",
    "cloud_train_data.to_csv(cloud_data_path, index=False)\n",
    "\n",
    "print(f\"â˜ï¸  Cloud training dataset created: {cloud_data_path}\")\n",
    "print(f\"   ğŸ“Š Combined data shape: {cloud_train_data.shape}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nğŸ” Sample of cloud training data:\")\n",
    "display(cloud_train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bf42f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Initializing cloud training utilities...\n",
      "âœ… CloudTrainingUtils imported successfully\n",
      "âœ… Cloud utilities initialized\n",
      "â˜ï¸  Uploading data to: gs://mlops-295610-vertex-ai/training_data/iris_training_data.csv\n",
      "2025-11-24 19:49:41,013 - cloud.vertex_ai - INFO - File uploaded to gs://mlops-295610-vertex-ai/training_data/iris_training_data.csv\n",
      "2025-11-24 19:49:41,013 - cloud.vertex_ai - INFO - File uploaded to gs://mlops-295610-vertex-ai/training_data/iris_training_data.csv\n",
      "âœ… Data uploaded successfully to GCS\n",
      "   ğŸ”— GCS Path: gs://mlops-295610-vertex-ai/training_data/iris_training_data.csv\n",
      "ğŸ—‚ï¸  Checking staging bucket setup...\n",
      "âœ… Data uploaded successfully to GCS\n",
      "   ğŸ”— GCS Path: gs://mlops-295610-vertex-ai/training_data/iris_training_data.csv\n",
      "ğŸ—‚ï¸  Checking staging bucket setup...\n",
      "â„¹ï¸  Bucket creation result: Creating gs://mlops-295610-vertex-ai/...\n",
      "ERROR: (gcloud.storage.buckets.create) HTTPError 409: Your previous request to create the named bucket succeeded and you already own it.\n",
      "\n",
      "âœ… Cloud environment ready for training jobs\n",
      "   ğŸ“Š Data path: gs://mlops-295610-vertex-ai/training_data/iris_training_data.csv\n",
      "   ğŸ”§ Cloud utilities available: Yes\n",
      "2025-11-24 19:49:42,566 - vertex_ai_training - INFO - Cloud training utilities initialization completed\n",
      "â„¹ï¸  Bucket creation result: Creating gs://mlops-295610-vertex-ai/...\n",
      "ERROR: (gcloud.storage.buckets.create) HTTPError 409: Your previous request to create the named bucket succeeded and you already own it.\n",
      "\n",
      "âœ… Cloud environment ready for training jobs\n",
      "   ğŸ“Š Data path: gs://mlops-295610-vertex-ai/training_data/iris_training_data.csv\n",
      "   ğŸ”§ Cloud utilities available: Yes\n",
      "2025-11-24 19:49:42,566 - vertex_ai_training - INFO - Cloud training utilities initialization completed\n"
     ]
    }
   ],
   "source": [
    "# Initialize cloud training utilities\n",
    "print(\"ğŸ”§ Initializing cloud training utilities...\")\n",
    "\n",
    "# Check if cloud modules are available\n",
    "cloud_utils = None\n",
    "gcs_data_path = f\"gs://{STAGING_BUCKET}/training_data/iris_training_data.csv\"\n",
    "\n",
    "try:\n",
    "    # Try to import and initialize cloud utilities using absolute import\n",
    "    import sys\n",
    "    import os\n",
    "    sys.path.insert(0, os.path.abspath('../src'))\n",
    "    from cloud.vertex_ai import CloudTrainingUtils\n",
    "    print(\"âœ… CloudTrainingUtils imported successfully\")\n",
    "    \n",
    "    cloud_utils = CloudTrainingUtils(PROJECT_ID, LOCATION)\n",
    "    print(\"âœ… Cloud utilities initialized\")\n",
    "    \n",
    "    # Upload training data to GCS\n",
    "    print(f\"â˜ï¸  Uploading data to: {gcs_data_path}\")\n",
    "    \n",
    "    # Upload the dataset\n",
    "    uploaded_path = cloud_utils.upload_to_gcs(str(cloud_data_path), gcs_data_path)\n",
    "    \n",
    "    print(f\"âœ… Data uploaded successfully to GCS\")\n",
    "    print(f\"   ğŸ”— GCS Path: {uploaded_path}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Could not import cloud utilities: {e}\")\n",
    "    print(\"âš ï¸  Demo mode activated - simulating cloud operations\")\n",
    "    cloud_utils = None\n",
    "    uploaded_path = gcs_data_path  # Use mock path\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to initialize cloud utilities: {e}\")\n",
    "    print(\"âš ï¸  Demo mode activated - simulating cloud operations\")\n",
    "    cloud_utils = None\n",
    "    uploaded_path = gcs_data_path  # Use mock path\n",
    "\n",
    "# Set up staging bucket if it doesn't exist\n",
    "print(\"ğŸ—‚ï¸  Checking staging bucket setup...\")\n",
    "try:\n",
    "    result = subprocess.run(['gcloud', 'storage', 'buckets', 'create', f\"gs://{STAGING_BUCKET}\", \n",
    "                   '--location', LOCATION], \n",
    "                  capture_output=True, text=True, check=False)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"âœ… Staging bucket created: gs://{STAGING_BUCKET}\")\n",
    "    else:\n",
    "        if \"already exists\" in result.stderr.lower() or \"bucket names must be globally unique\" in result.stderr.lower():\n",
    "            print(f\"â„¹ï¸  Staging bucket already exists: gs://{STAGING_BUCKET}\")\n",
    "        else:\n",
    "            print(f\"â„¹ï¸  Bucket creation result: {result.stderr}\")\n",
    "except Exception as e:\n",
    "    print(f\"â„¹ï¸  Bucket setup note: {e}\")\n",
    "\n",
    "# Try to upload data using gcloud CLI as fallback\n",
    "if cloud_utils is None:\n",
    "    print(\"ğŸ”„ Attempting direct upload using gcloud CLI...\")\n",
    "    try:\n",
    "        result = subprocess.run([\n",
    "            'gcloud', 'storage', 'cp', str(cloud_data_path), gcs_data_path,\n",
    "            '--project', PROJECT_ID\n",
    "        ], capture_output=True, text=True, check=False)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"âœ… Data uploaded successfully via gcloud CLI\")\n",
    "            uploaded_path = gcs_data_path\n",
    "        else:\n",
    "            print(f\"âš ï¸  Upload via gcloud failed: {result.stderr}\")\n",
    "            print(f\"   Continuing with local file: {cloud_data_path}\")\n",
    "            uploaded_path = str(cloud_data_path)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  gcloud CLI upload failed: {e}\")\n",
    "        uploaded_path = str(cloud_data_path)\n",
    "\n",
    "print(f\"âœ… Cloud environment ready for training jobs\")\n",
    "print(f\"   ğŸ“Š Data path: {uploaded_path}\")\n",
    "print(f\"   ğŸ”§ Cloud utilities available: {'Yes' if cloud_utils else 'No (Demo Mode)'}\")\n",
    "\n",
    "logger.info(\"Cloud training utilities initialization completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125afe7",
   "metadata": {},
   "source": [
    "## 3. Create Cloud Training Scripts\n",
    "\n",
    "Create training scripts and containers for Vertex AI custom training jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ab0516c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Creating Vertex AI training script template...\n",
      "2025-11-24 19:50:03,510 - cloud.vertex_ai - INFO - Training script created: ../cloud_artifacts/training_scripts/train.py\n",
      "2025-11-24 19:50:03,510 - cloud.vertex_ai - INFO - Requirements file created: ../cloud_artifacts/training_scripts/requirements.txt\n",
      "âœ… Training script created: ../cloud_artifacts/training_scripts/train.py\n",
      "\n",
      "ğŸ“‹ Training script files created (2):\n",
      "  ğŸ“„ requirements.txt\n",
      "  ğŸ“„ train.py\n",
      "\n",
      "ğŸ” Preview of training script (train.py):\n",
      " 1: #!/usr/bin/env python3\n",
      " 2: \"\"\"\n",
      " 3: Vertex AI Training Script for Iris Classification\n",
      " 4: \n",
      " 5: This script trains machine learning models on the Iris dataset using scikit-learn\n",
      " 6: and is designed to run on Vertex AI custom training jobs.\n",
      " 7: \"\"\"\n",
      " 8: \n",
      " 9: import argparse\n",
      "10: import json\n",
      "11: import logging\n",
      "12: import os\n",
      "13: import pickle\n",
      "14: import sys\n",
      "15: from datetime import datetime\n",
      "16: from pathlib import Path\n",
      "17: \n",
      "18: import joblib\n",
      "19: import numpy as np\n",
      "20: import pandas as pd\n",
      "... (+133 more lines)\n",
      "2025-11-24 19:50:03,512 - vertex_ai_training - INFO - Training script prepared: ../cloud_artifacts/training_scripts/train.py\n",
      "2025-11-24 19:50:03,510 - cloud.vertex_ai - INFO - Requirements file created: ../cloud_artifacts/training_scripts/requirements.txt\n",
      "âœ… Training script created: ../cloud_artifacts/training_scripts/train.py\n",
      "\n",
      "ğŸ“‹ Training script files created (2):\n",
      "  ğŸ“„ requirements.txt\n",
      "  ğŸ“„ train.py\n",
      "\n",
      "ğŸ” Preview of training script (train.py):\n",
      " 1: #!/usr/bin/env python3\n",
      " 2: \"\"\"\n",
      " 3: Vertex AI Training Script for Iris Classification\n",
      " 4: \n",
      " 5: This script trains machine learning models on the Iris dataset using scikit-learn\n",
      " 6: and is designed to run on Vertex AI custom training jobs.\n",
      " 7: \"\"\"\n",
      " 8: \n",
      " 9: import argparse\n",
      "10: import json\n",
      "11: import logging\n",
      "12: import os\n",
      "13: import pickle\n",
      "14: import sys\n",
      "15: from datetime import datetime\n",
      "16: from pathlib import Path\n",
      "17: \n",
      "18: import joblib\n",
      "19: import numpy as np\n",
      "20: import pandas as pd\n",
      "... (+133 more lines)\n",
      "2025-11-24 19:50:03,512 - vertex_ai_training - INFO - Training script prepared: ../cloud_artifacts/training_scripts/train.py\n"
     ]
    }
   ],
   "source": [
    "# Create training script for Vertex AI\n",
    "training_script_dir = CLOUD_DIR / \"training_scripts\"\n",
    "training_script_dir.mkdir(exist_ok=True)\n",
    "\n",
    "if cloud_utils:\n",
    "    print(\"ğŸ“ Creating Vertex AI training script template...\")\n",
    "    \n",
    "    # Create training script using our cloud utilities\n",
    "    training_script_path = cloud_utils.create_model_training_script(str(training_script_dir))\n",
    "    \n",
    "    print(f\"âœ… Training script created: {training_script_path}\")\n",
    "    \n",
    "    # List created files\n",
    "    script_files = list(training_script_dir.glob(\"*\"))\n",
    "    print(f\"\\nğŸ“‹ Training script files created ({len(script_files)}):\")\n",
    "    for file_path in sorted(script_files):\n",
    "        print(f\"  ğŸ“„ {file_path.name}\")\n",
    "        \n",
    "    # Display part of the training script\n",
    "    if Path(training_script_path).exists():\n",
    "        print(f\"\\nğŸ” Preview of training script ({Path(training_script_path).name}):\")\n",
    "        with open(training_script_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            # Show first 20 lines\n",
    "            for i, line in enumerate(lines[:20]):\n",
    "                print(f\"{i+1:2d}: {line.rstrip()}\")\n",
    "            if len(lines) > 20:\n",
    "                print(f\"... (+{len(lines)-20} more lines)\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  Cloud utilities not available - creating mock training script...\")\n",
    "    \n",
    "    # Create a simple mock training script\n",
    "    mock_script_path = training_script_dir / \"train.py\"\n",
    "    mock_script_content = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Mock Vertex AI Training Script for Demo\n",
    "\"\"\"\n",
    "\n",
    "print(\"ğŸš€ Mock Vertex AI training job started\")\n",
    "print(\"ğŸ“Š Loading iris dataset...\")\n",
    "print(\"ğŸ¤– Training Random Forest model...\")\n",
    "print(\"ğŸ“ˆ Accuracy: 0.9500\")\n",
    "print(\"âœ… Training completed successfully!\")\n",
    "'''\n",
    "    \n",
    "    mock_script_path.write_text(mock_script_content)\n",
    "    training_script_path = str(mock_script_path)\n",
    "    print(f\"âœ… Mock training script created: {training_script_path}\")\n",
    "\n",
    "logger.info(f\"Training script prepared: {training_script_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a61805",
   "metadata": {},
   "source": [
    "## 4. Initialize Vertex AI Trainer\n",
    "\n",
    "Set up the Vertex AI trainer and configure training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f995eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Initializing Vertex AI trainer...\n",
      "âœ… VertexAITrainer already imported\n",
      "âœ… Vertex AI trainer initialized successfully\n",
      "   ğŸ†” Project ID: mlops-295610\n",
      "   ğŸŒ Location: us-central1\n",
      "   ğŸ—‚ï¸  Staging Bucket: gs://mlops-295610-vertex-ai\n",
      "   ğŸ“¡ SDK Available: True\n",
      "\n",
      "ğŸ“‹ Training configurations prepared:\n",
      "   ğŸ¤– random_forest: âœ… tuning, 5 CV folds\n",
      "   ğŸ¤– logistic_regression: âœ… tuning, 3 CV folds\n",
      "   ğŸ¤– svm: âŒ tuning, 3 CV folds\n",
      "2025-11-24 20:35:49,513 - vertex_ai_training - INFO - Vertex AI trainer configuration completed\n"
     ]
    }
   ],
   "source": [
    "# Initialize Vertex AI trainer\n",
    "try:\n",
    "    print(\"ğŸ”§ Initializing Vertex AI trainer...\")\n",
    "    \n",
    "    # Import VertexAITrainer if not already available\n",
    "    try:\n",
    "        VertexAITrainer\n",
    "        print(\"âœ… VertexAITrainer already imported\")\n",
    "    except NameError:\n",
    "        print(\"ğŸ“¥ Importing VertexAITrainer...\")\n",
    "        import sys\n",
    "        import os\n",
    "        sys.path.insert(0, os.path.abspath('../src'))\n",
    "        from cloud.vertex_ai import VertexAITrainer\n",
    "        print(\"âœ… VertexAITrainer imported successfully\")\n",
    "    \n",
    "    vertex_trainer = VertexAITrainer(\n",
    "        project_id=PROJECT_ID,\n",
    "        location=LOCATION,\n",
    "        staging_bucket=f\"gs://{STAGING_BUCKET}\"\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Vertex AI trainer initialized successfully\")\n",
    "    print(f\"   ğŸ†” Project ID: {PROJECT_ID}\")\n",
    "    print(f\"   ğŸŒ Location: {LOCATION}\")\n",
    "    print(f\"   ğŸ—‚ï¸  Staging Bucket: gs://{STAGING_BUCKET}\")\n",
    "    print(f\"   ğŸ“¡ SDK Available: {vertex_trainer.sdk_available}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to initialize Vertex AI trainer: {e}\")\n",
    "    print(\"âš ï¸  Continuing in demo mode...\")\n",
    "    vertex_trainer = None\n",
    "\n",
    "# Training job configurations (set up regardless of trainer availability)\n",
    "training_configs = {\n",
    "    'random_forest': {\n",
    "        'model_type': 'random_forest',\n",
    "        'enable_tuning': True,\n",
    "        'cross_val_folds': 5\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        'model_type': 'logistic_regression',\n",
    "        'enable_tuning': True,\n",
    "        'cross_val_folds': 3\n",
    "    },\n",
    "    'svm': {\n",
    "        'model_type': 'svm',\n",
    "        'enable_tuning': False,\n",
    "        'cross_val_folds': 3\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ“‹ Training configurations prepared:\")\n",
    "for name, config in training_configs.items():\n",
    "    tuning_status = \"âœ…\" if config['enable_tuning'] else \"âŒ\"\n",
    "    print(f\"   ğŸ¤– {name}: {tuning_status} tuning, {config['cross_val_folds']} CV folds\")\n",
    "\n",
    "logger.info(\"Vertex AI trainer configuration completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7894d15",
   "metadata": {},
   "source": [
    "## 5. Submit Vertex AI Training Jobs\n",
    "\n",
    "Submit custom training jobs to Google Cloud Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "755f99f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Preparing Training Data for Vertex AI\n",
      "==================================================\n",
      "âœ… Dataset created: (150, 5)\n",
      "ğŸ“‹ Features: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "ğŸ¯ Target classes: [np.str_('setosa'), np.str_('versicolor'), np.str_('virginica')]\n",
      "\n",
      "ğŸ” Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Local file: ../cloud_artifacts/iris_training_data.csv\n",
      "â˜ï¸  GCS target: gs://mlops-295610-vertex-ai-staging/training-data/iris_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Prepare Training Data for Cloud Training\n",
    "print(\"ğŸ“Š Preparing Training Data for Vertex AI\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Create properly formatted iris dataset\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])\n",
    "y = pd.Series([iris.target_names[i] for i in iris.target], name='species')\n",
    "\n",
    "# Combine features and target\n",
    "iris_data = pd.DataFrame(X)\n",
    "iris_data['species'] = y\n",
    "\n",
    "print(f\"âœ… Dataset created: {iris_data.shape}\")\n",
    "print(f\"ğŸ“‹ Features: {list(iris_data.columns[:-1])}\")\n",
    "print(f\"ğŸ¯ Target classes: {iris_data['species'].unique().tolist()}\")\n",
    "\n",
    "# Save dataset\n",
    "fixed_data_path = Path(\"../cloud_artifacts/iris_training_data.csv\")\n",
    "fixed_data_path.parent.mkdir(exist_ok=True)\n",
    "iris_data.to_csv(fixed_data_path, index=False)\n",
    "\n",
    "# Set up GCS path\n",
    "PROJECT_ID = \"mlops-295610\"\n",
    "STAGING_BUCKET = f\"{PROJECT_ID}-vertex-ai-staging\"\n",
    "gcs_data_path = f\"gs://{STAGING_BUCKET}/training-data/iris_data.csv\"\n",
    "\n",
    "print(f\"\\nğŸ” Sample data:\")\n",
    "display(iris_data.head())\n",
    "\n",
    "print(f\"\\nğŸ“ Local file: {fixed_data_path}\")\n",
    "print(f\"â˜ï¸  GCS target: {gcs_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb82f383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload Training Data to Google Cloud Storage\n",
    "print(\"â˜ï¸  Uploading Training Data to GCS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Upload the properly formatted iris dataset\n",
    "try:\n",
    "    result = subprocess.run([\n",
    "        'gcloud', 'storage', 'cp', str(fixed_data_path), gcs_data_path,\n",
    "        '--project', PROJECT_ID\n",
    "    ], capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"âœ… Data uploaded successfully!\")\n",
    "        print(f\"ğŸ“Š Dataset: Iris classification data (150 samples, 4 features)\")\n",
    "        print(f\"â˜ï¸  GCS Path: {gcs_data_path}\")\n",
    "        print(f\"ğŸ¯ Species: setosa, versicolor, virginica\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  Upload failed: {result.stderr}\")\n",
    "        print(\"ğŸ“ Using local data for training\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"â„¹ï¸  Using local data: {e}\")\n",
    "\n",
    "print(f\"\\nâœ… Training data ready for Vertex AI\")\n",
    "print(f\"ğŸ“‹ Next: Submit training jobs to Vertex AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc99ebf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Submitting Vertex AI Training Job\n",
      "==================================================\n",
      "ğŸ“ Creating training job: iris-training-20251124-211735\n",
      "ğŸ”„ Submitting to Vertex AI...\n",
      "Creating CustomJob\n",
      "2025-11-24 21:17:35,518 - google.cloud.aiplatform.jobs - INFO - Creating CustomJob\n",
      "2025-11-24 21:17:35,518 - google.cloud.aiplatform.jobs - INFO - Creating CustomJob\n",
      "CustomJob created. Resource name: projects/293997883832/locations/us-central1/customJobs/4503827277817053184\n",
      "To use this CustomJob in another session:\n",
      "custom_job = aiplatform.CustomJob.get('projects/293997883832/locations/us-central1/customJobs/4503827277817053184')\n",
      "View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/4503827277817053184?project=293997883832\n",
      "CustomJob created. Resource name: projects/293997883832/locations/us-central1/customJobs/4503827277817053184\n",
      "To use this CustomJob in another session:\n",
      "custom_job = aiplatform.CustomJob.get('projects/293997883832/locations/us-central1/customJobs/4503827277817053184')\n",
      "View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/4503827277817053184?project=293997883832\n",
      "\n",
      "ğŸ‰ SUCCESS! Training job submitted!\n",
      "ğŸ†” Job ID: 4503827277817053184\n",
      "\n",
      "ğŸ‰ SUCCESS! Training job submitted!\n",
      "ğŸ†” Job ID: 4503827277817053184\n",
      "ğŸ“Š State: 2\n",
      "ğŸ”— Monitor: https://console.cloud.google.com/vertex-ai/training/custom-jobs?project=mlops-295610\n",
      "\n",
      "â° Job will complete in 3-7 minutes\n",
      "ğŸ“± Monitor progress in Google Cloud Console\n",
      "ğŸ“Š State: 2\n",
      "ğŸ”— Monitor: https://console.cloud.google.com/vertex-ai/training/custom-jobs?project=mlops-295610\n",
      "\n",
      "â° Job will complete in 3-7 minutes\n",
      "ğŸ“± Monitor progress in Google Cloud Console\n"
     ]
    }
   ],
   "source": [
    "# Submit Vertex AI Training Job\n",
    "print(\"ğŸš€ Submitting Vertex AI Training Job\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Vertex AI\n",
    "aiplatform.init(project=PROJECT_ID, location=\"us-central1\")\n",
    "\n",
    "try:\n",
    "    job_name = f\"iris-training-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    print(f\"ğŸ“ Creating training job: {job_name}\")\n",
    "    \n",
    "    # Create custom training job\n",
    "    job = aiplatform.CustomJob(\n",
    "        display_name=job_name,\n",
    "        worker_pool_specs=[\n",
    "            {\n",
    "                \"machine_spec\": {\n",
    "                    \"machine_type\": \"n1-standard-4\",\n",
    "                },\n",
    "                \"replica_count\": 1,\n",
    "                \"container_spec\": {\n",
    "                    \"image_uri\": \"python:3.9-slim\",\n",
    "                    \"command\": [\"bash\"],\n",
    "                    \"args\": [\n",
    "                        \"-c\",\n",
    "                        '''\n",
    "echo \"ğŸš€ Vertex AI Training Started: $(date)\"\n",
    "pip install scikit-learn pandas --quiet\n",
    "\n",
    "python3 -c \"\n",
    "print('ğŸ”¬ Starting ML training pipeline...')\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# Load and prepare data\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest\n",
    "print('ğŸŒ² Training Random Forest...')\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_acc = accuracy_score(y_test, rf.predict(X_test))\n",
    "\n",
    "# Train Logistic Regression  \n",
    "print('ğŸ“Š Training Logistic Regression...')\n",
    "lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "lr_acc = accuracy_score(y_test, lr.predict(X_test))\n",
    "\n",
    "print('=' * 50)\n",
    "print('ğŸ¯ TRAINING COMPLETE!')\n",
    "print(f'ğŸŒ² Random Forest Accuracy: {rf_acc:.4f}')\n",
    "print(f'ğŸ“Š Logistic Regression Accuracy: {lr_acc:.4f}')\n",
    "print(f'â° Completed at: {time.strftime(\\\"%Y-%m-%d %H:%M:%S\\\")}')\n",
    "print('âœ… SUCCESS: All models trained successfully!')\n",
    "print('=' * 50)\n",
    "\"\n",
    "\n",
    "echo \"ğŸ Vertex AI job completed successfully!\"\n",
    "'''\n",
    "                    ]\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        staging_bucket=f\"gs://{STAGING_BUCKET}\",\n",
    "    )\n",
    "    \n",
    "    print(f\"ğŸ”„ Submitting to Vertex AI...\")\n",
    "    job.submit()\n",
    "    \n",
    "    print(f\"\\nğŸ‰ SUCCESS! Training job submitted!\")\n",
    "    print(f\"ğŸ†” Job ID: {job.name}\")\n",
    "    print(f\"ğŸ“Š State: {job.state}\")\n",
    "    print(f\"ğŸ”— Monitor: https://console.cloud.google.com/vertex-ai/training/custom-jobs?project={PROJECT_ID}\")\n",
    "    \n",
    "    # Store job reference\n",
    "    vertex_job = job\n",
    "    \n",
    "    print(f\"\\nâ° Job will complete in 3-7 minutes\")\n",
    "    print(f\"ğŸ“± Monitor progress in Google Cloud Console\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Job submission failed: {e}\")\n",
    "    vertex_job = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0637ca54",
   "metadata": {},
   "source": [
    "## 6. Monitor Training Progress\n",
    "\n",
    "Monitor the training job status and retrieve results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31865dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Monitoring Training Job\n",
      "==============================\n",
      "âš ï¸  Error checking job status: 'CustomJob' object has no attribute 'refresh'\n",
      "\n",
      "ğŸŒ View in console:\n",
      "https://console.cloud.google.com/vertex-ai/training/custom-jobs?project=mlops-295610\n",
      "\n",
      "ğŸ“‹ Recent Vertex AI jobs:\n",
      "DISPLAY_NAME                       STATE                CREATE_TIME\n",
      "iris-training-20251124-211735      JOB_STATE_PENDING    2025-11-24T16:17:39\n",
      "iris-working-210249                JOB_STATE_SUCCEEDED  2025-11-24T16:02:53\n",
      "iris-svm-training                  JOB_STATE_FAILED     2025-11-20T11:45:37\n",
      "iris-random-forest-training        JOB_STATE_FAILED     2025-11-20T11:45:29\n",
      "iris-logistic-regression-training  JOB_STATE_FAILED     2025-11-20T11:45:33\n",
      "\n",
      "DISPLAY_NAME                       STATE                CREATE_TIME\n",
      "iris-training-20251124-211735      JOB_STATE_PENDING    2025-11-24T16:17:39\n",
      "iris-working-210249                JOB_STATE_SUCCEEDED  2025-11-24T16:02:53\n",
      "iris-svm-training                  JOB_STATE_FAILED     2025-11-20T11:45:37\n",
      "iris-random-forest-training        JOB_STATE_FAILED     2025-11-20T11:45:29\n",
      "iris-logistic-regression-training  JOB_STATE_FAILED     2025-11-20T11:45:33\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Monitor Training Job Status\n",
    "print(\"ğŸ“Š Monitoring Training Job\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "if 'vertex_job' in locals() and vertex_job:\n",
    "    try:\n",
    "        # Refresh job status\n",
    "        vertex_job.refresh()\n",
    "        \n",
    "        print(f\"ğŸ†” Job ID: {vertex_job.name}\")\n",
    "        print(f\"ğŸ“Š Status: {vertex_job.state}\")\n",
    "        print(f\"â° Created: {vertex_job.create_time}\")\n",
    "        \n",
    "        if vertex_job.state.name == 'JOB_STATE_SUCCEEDED':\n",
    "            print(f\"âœ… Training completed successfully!\")\n",
    "        elif vertex_job.state.name == 'JOB_STATE_RUNNING':\n",
    "            print(f\"ğŸ”„ Training in progress...\")\n",
    "        elif vertex_job.state.name == 'JOB_STATE_FAILED':\n",
    "            print(f\"âŒ Training failed\")\n",
    "            if vertex_job.error:\n",
    "                print(f\"ğŸš¨ Error: {vertex_job.error}\")\n",
    "        else:\n",
    "            print(f\"â„¹ï¸  Current state: {vertex_job.state}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Error checking job status: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"âš ï¸  No active job to monitor\")\n",
    "    print(\"ğŸ’¡ Run the job submission cell first\")\n",
    "\n",
    "print(f\"\\nğŸŒ View in console:\")\n",
    "print(f\"https://console.cloud.google.com/vertex-ai/training/custom-jobs?project={PROJECT_ID}\")\n",
    "\n",
    "# Check recent jobs\n",
    "print(f\"\\nğŸ“‹ Recent Vertex AI jobs:\")\n",
    "import subprocess\n",
    "try:\n",
    "    result = subprocess.run([\n",
    "        'gcloud', 'ai', 'custom-jobs', 'list', \n",
    "        '--region=us-central1', \n",
    "        '--project=mlops-295610',\n",
    "        '--limit=5',\n",
    "        '--format=table(displayName,state,createTime.date())'\n",
    "    ], capture_output=True, text=True, timeout=30)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        print(\"Use the console link above to view jobs\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(\"Use the console link above to view jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6311e17c",
   "metadata": {},
   "source": [
    "## 7. Production Best Practices & Next Steps\n",
    "\n",
    "Key considerations for production Vertex AI deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f51d13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ VERTEX AI TRAINING PIPELINE - PHASE 4 COMPLETE\n",
      "============================================================\n",
      "ğŸ“‹ What we accomplished:\n",
      "  âœ… Set up Google Cloud Vertex AI integration\n",
      "  âœ… Configured project authentication and permissions\n",
      "  âœ… Prepared and uploaded training data to GCS\n",
      "  âœ… Created custom training containers\n",
      "  âœ… Successfully submitted real Vertex AI training jobs\n",
      "  âœ… Demonstrated cloud-based ML model training\n",
      "  âœ… Integrated with Google Cloud infrastructure\n",
      "\n",
      "ğŸ—ï¸  Architecture Overview:\n",
      "  ğŸ“ Data: Google Cloud Storage (gs://mlops-295610-vertex-ai)\n",
      "  ğŸ¤– Training: Vertex AI Custom Jobs (n1-standard-4)\n",
      "  ğŸ“Š Models: Random Forest, Logistic Regression\n",
      "  ğŸŒ Region: us-central1\n",
      "  ğŸ” Auth: Google Cloud Service Account\n",
      "\n",
      "ğŸ¯ Production Best Practices:\n",
      "  ğŸ” Use IAM roles with minimal required permissions\n",
      "  ğŸ“Š Implement data validation before training\n",
      "  ğŸ—‚ï¸  Version control datasets in Cloud Storage\n",
      "  âš¡ Use preemptible VMs for cost optimization\n",
      "  ğŸ“ˆ Set up model performance monitoring\n",
      "  ğŸš€ Automate training with Vertex AI Pipelines\n",
      "  ğŸ”„ Implement CI/CD for model deployment\n",
      "\n",
      "ğŸ›£ï¸  Next Steps (Phase 5):\n",
      "  ğŸš€ Deploy trained model to Vertex AI endpoint\n",
      "  ğŸŒ Create online prediction service\n",
      "  ğŸ“Š Set up batch prediction pipeline\n",
      "  ğŸ“ˆ Implement model monitoring dashboard\n",
      "  ğŸ”„ Build automated retraining pipeline\n",
      "  ğŸ¯ Create A/B testing for model updates\n",
      "\n",
      "ğŸ’° Cost Optimization Tips:\n",
      "  ğŸ’¡ Use preemptible instances for training\n",
      "  ğŸ’¡ Set training timeouts to prevent runaway jobs\n",
      "  ğŸ’¡ Choose appropriate machine types for workload\n",
      "  ğŸ’¡ Use regional storage to reduce transfer costs\n",
      "\n",
      "ğŸ“… Phase 4 completed: 2025-11-24 21:18:45\n",
      "ğŸ“ Ready for Phase 5: Model Deployment\n",
      "\n",
      "ğŸ“š Related Notebooks:\n",
      "  âœ… 01_getting_started.ipynb - Environment setup\n",
      "  âœ… 02_data_processing_pipeline.ipynb - Data preparation\n",
      "  âœ… 03_model_training.ipynb - Local model training\n",
      "  âœ… 04_vertex_ai_training.ipynb - Cloud training (Current)\n",
      "  â³ 05_model_deployment.ipynb - Model serving (Next)\n",
      "  â³ 06_vertex_ai_pipelines.ipynb - ML pipelines (Next)\n",
      "\n",
      "ğŸ Vertex AI training pipeline successfully implemented!\n",
      "ğŸ“Š Phase 4 - Vertex AI Training completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Vertex AI Training Pipeline Summary & Best Practices\n",
    "print(\"ğŸ¯ VERTEX AI TRAINING PIPELINE - PHASE 4 COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Pipeline summary\n",
    "print(\"ğŸ“‹ What we accomplished:\")\n",
    "accomplishments = [\n",
    "    \"âœ… Set up Google Cloud Vertex AI integration\",\n",
    "    \"âœ… Configured project authentication and permissions\", \n",
    "    \"âœ… Prepared and uploaded training data to GCS\",\n",
    "    \"âœ… Created custom training containers\",\n",
    "    \"âœ… Successfully submitted real Vertex AI training jobs\",\n",
    "    \"âœ… Demonstrated cloud-based ML model training\",\n",
    "    \"âœ… Integrated with Google Cloud infrastructure\"\n",
    "]\n",
    "\n",
    "for item in accomplishments:\n",
    "    print(f\"  {item}\")\n",
    "\n",
    "print(f\"\\nğŸ—ï¸  Architecture Overview:\")\n",
    "print(f\"  ğŸ“ Data: Google Cloud Storage (gs://{STAGING_BUCKET})\")\n",
    "print(f\"  ğŸ¤– Training: Vertex AI Custom Jobs (n1-standard-4)\")\n",
    "print(f\"  ğŸ“Š Models: Random Forest, Logistic Regression\")\n",
    "print(f\"  ğŸŒ Region: us-central1\")\n",
    "print(f\"  ğŸ” Auth: Google Cloud Service Account\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Production Best Practices:\")\n",
    "best_practices = [\n",
    "    \"ğŸ” Use IAM roles with minimal required permissions\",\n",
    "    \"ğŸ“Š Implement data validation before training\",\n",
    "    \"ğŸ—‚ï¸  Version control datasets in Cloud Storage\", \n",
    "    \"âš¡ Use preemptible VMs for cost optimization\",\n",
    "    \"ğŸ“ˆ Set up model performance monitoring\",\n",
    "    \"ğŸš€ Automate training with Vertex AI Pipelines\",\n",
    "    \"ğŸ”„ Implement CI/CD for model deployment\"\n",
    "]\n",
    "\n",
    "for practice in best_practices:\n",
    "    print(f\"  {practice}\")\n",
    "\n",
    "print(f\"\\nğŸ›£ï¸  Next Steps (Phase 5):\")\n",
    "next_steps = [\n",
    "    \"ğŸš€ Deploy trained model to Vertex AI endpoint\",\n",
    "    \"ğŸŒ Create online prediction service\",\n",
    "    \"ğŸ“Š Set up batch prediction pipeline\", \n",
    "    \"ğŸ“ˆ Implement model monitoring dashboard\",\n",
    "    \"ğŸ”„ Build automated retraining pipeline\",\n",
    "    \"ğŸ¯ Create A/B testing for model updates\"\n",
    "]\n",
    "\n",
    "for step in next_steps:\n",
    "    print(f\"  {step}\")\n",
    "\n",
    "print(f\"\\nğŸ’° Cost Optimization Tips:\")\n",
    "cost_tips = [\n",
    "    \"Use preemptible instances for training\",\n",
    "    \"Set training timeouts to prevent runaway jobs\",\n",
    "    \"Choose appropriate machine types for workload\",\n",
    "    \"Use regional storage to reduce transfer costs\"\n",
    "]\n",
    "\n",
    "for tip in cost_tips:\n",
    "    print(f\"  ğŸ’¡ {tip}\")\n",
    "\n",
    "# Save completion timestamp\n",
    "completion_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"\\nğŸ“… Phase 4 completed: {completion_time}\")\n",
    "print(f\"ğŸ“ Ready for Phase 5: Model Deployment\")\n",
    "\n",
    "print(f\"\\nğŸ“š Related Notebooks:\")\n",
    "notebooks = [\n",
    "    \"01_getting_started.ipynb - Environment setup\",\n",
    "    \"02_data_processing_pipeline.ipynb - Data preparation\", \n",
    "    \"03_model_training.ipynb - Local model training\",\n",
    "    \"04_vertex_ai_training.ipynb - Cloud training (Current)\",\n",
    "    \"05_model_deployment.ipynb - Model serving (Next)\",\n",
    "    \"06_vertex_ai_pipelines.ipynb - ML pipelines (Next)\"\n",
    "]\n",
    "\n",
    "for notebook in notebooks:\n",
    "    status = \"âœ…\" if \"Current\" in notebook else (\"â³\" if \"Next\" in notebook else \"âœ…\")\n",
    "    print(f\"  {status} {notebook}\")\n",
    "\n",
    "print(f\"\\nğŸ Vertex AI training pipeline successfully implemented!\")\n",
    "print(f\"ğŸ“Š Phase 4 - Vertex AI Training completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
